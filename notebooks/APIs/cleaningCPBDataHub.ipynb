{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a database of the various measurements for benthic sediment, waterTidalBenthic quality, and biomass in the Chesapeake Bay. Due to the organization of the data, the measured parameters are in a column. In the [Preparing to combine datasets section](#preparing-to-combine-datasets) we will [turn parameters into columns](#turn-parameters-into-columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSVs for both the Tidal Benthic database and thhe Water Quakity database have the following columns. When different names are used for the same information across different datasets, all names are included. These differences will be handked in cleaning.\n",
    "\n",
    "Here is a descriptor of the columns, from [The 2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf) and [Guide to Using Chesapeake Bay Program Water Quality Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/wq_data_userguide_10feb12_mod.pdf):\n",
    "- `CBSeg2003` 2003 Chesapeake Bay Segment Designation. Divided into regions based on circulation and salinity properties. We used 8 from the Bay proper, 2 adjoining Bays, and 1 adjoining sound.\n",
    "- `CBSeg2003Description` 2003 Chesapeake Bay Segment Designation Description in the format Location-Salinity. The locations are Chesapeake Bay, Eastern Bay, Mobjack Bay, and Tangier Sound. The salinity levels are tidal fresh (0.0 - 0.5 parts per thousand),\n",
    "oligohaline (0.5 - 5.0 parts per thousand), mesohaline (5.0 - 18.0 parts per thousand), and polyhaline (greater than 18.0 parts per thousand). \n",
    "- `Station` the sampling station\n",
    "- `TotalDepth` is Total *Station* Depth (Meters)\n",
    "- `Source` the s the agency or company that collected the water quality samples or took the field measurements\n",
    "- `Latitude` and  `Longitude`, the Latitude and Longitude for the sampling station\n",
    "- `FieldActivityId` (tidal  benthic database) and `EventId` (Water Quality database), are Database Generated Event Identification Numbers, mostly added from the monitoring event data.\n",
    "- `EventId` (sediment, water, taxonomic) and `BiologicalEventId` (biomass, ibi) encode both the `FieldActivityId` and sampling information. The same time, date, location will have one `FieldActivityId` but possibly multpile `EventId` or `BiologicalEventId`.\n",
    "- `SampleDate` Sampling date (MM/DD/YYYY). \n",
    "- `SampleTime` Sample Collection Time (HH:MM)\n",
    "- `Layer` Layer of water column in which sample was taken. However, this column is not consistently coded in the taxonomic counts data.\n",
    "- `SampleReplicate` This parameter combines the sample replicate number with a sample type\n",
    "descriptor. \n",
    "     - S1, Sample 1. The vast majority of the data.\n",
    "     - S2, Sample 2 \n",
    "- `ReportingParameter` (sediment), `ReportedParameter` (water quality), `IBIParameter` (biomass, indicator of benthic integrity), `Parameter` (Water Quality database) Sampling Parameter.\n",
    "- `ReportingValue` (taxonomic), `ReportedValue` (sediment, water quality), `IBIValue` (biomass, indicator of benthic integrity), `MeasureValue` (Water Quality database) the value of the parameter.\n",
    "- `ReportingUnits` (sediment, taxonomic), `ReportedUnits` (water quality), `Unit` (Water Quality Database). This parameter describes the units in which a substance is measured. \n",
    "- `ProjectIdentifier` (Tidal Benthic Database) or `Project` (Water Quality database), abbreviation for the project that collected the data.\n",
    "\n",
    "\n",
    "The Tidal Benthic data also has:\n",
    "- `Units` units for the sample volume. Always centimeters (cubic centimeters?).\n",
    "- `SampleVolume` Total Volume of Sample\n",
    "- `PDepth`, Composite Sample Cut Off Depth (meters)\n",
    "- `Salzone`, Salinity Zone\n",
    "     - P, Polyhaline =>18 parts per thousand\n",
    "     - HM, High Mesohaline =>12 TO 18 parts per thousand\n",
    "     - LM, Low Mesohaline =>5.0 TO 12 parts per thousand\n",
    "     - M, Mesohaline =>5.0 TO 18 parts per thousand\n",
    "     - O, Oligohaline =>0.5 TO 5.0 parts per thousand\n",
    "     - TF, Tidal Fresh < 0.5 parts per thousand\n",
    "\n",
    "Additionally, the taxonomic dataset has \n",
    "- `GMethod` Chesapeake Bay Program Gear Method Code. Codes represent information relating to the type of field gear used to collect samples for all analysis. In this dataset all are 7, Plankton Pump\n",
    "- `TSN` ITIS Taxon Serial Number, unique to the species. When used in conjunction with the NODC, the TSN\n",
    "overcomes the problem of numeric changes in the NODC code whenever species are reclassified. \n",
    "- `LatinName` Species Latin Name \n",
    "- `Size` Cell Size Groupings when taken. Some species have different measurements for different sizes. \n",
    "- `LifeStageDescription`, a numeric code of the life stage. Most are 89 - not specified.\n",
    "\n",
    "The Water Quality database also has:\n",
    "- `Cruise`, This alpha-numeric code identifies the cruise to which the data observation belongs. Cruise identification\n",
    "is useful for grouping data that are collected over a range of sample dates, but that are considered data\n",
    "for a specific sampling period. (Should be in both datasets)\n",
    "- `Program`, The PROGRAM code was added to the database design because Maryland DNR has adopted a\n",
    "project-oriented approach to water quality data management. This approach relies upon the use\n",
    "of PROGRAM (WQMP at DNR) and PROJECT (MAIN and TRIB) codes.\n",
    "- `Agency`, agencies that are ultimately responsible for ensuring the proper processing, storage, and submission or serving of water quality data\n",
    "- `UpperPycnocline`, Depth of upper pycnocline\n",
    "- `LowerPycnocline`, Depth of lower pycnocline\n",
    "- `Depth`\n",
    "- `Method`\n",
    "- `Lab`\n",
    "- `Problem`\n",
    "- `Details`\n",
    "- `TierLevel`\n",
    "- Weather information. The other than air temperature, these are encoded with the description, but most already have a numeric code in the [Guide to Using Chesapeake Bay Program Water Quality Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/wq_data_userguide_10feb12_mod.pdf)\n",
    "     - `AirTemp` in degrees Celcius\n",
    "     - `WindSpeed` the best estimate of the wind speed experienced during a sampling event, in a 10 knot range. Numerical dictionary available.\n",
    "     - `WindDirection` the prevailing wind direction experienced during a sampling event. \n",
    "     - `PrecipType` the type of precipitation experienced during a sampling event. Numerical dictionary available.\n",
    "     - `TideStage` the tidal stage experienced during a sampling event.\n",
    "     - `WaveHeight` the best estimate of the wave conditions experienced during a sampling event, as a range. Numerical dictionary available.\n",
    "     - `CloudCover` best estimate of the percent cloud cover experienced during a sampling event. Numerical dictionary available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Individual Datasets\n",
    "\n",
    "The first step of data cleaning is working with the individual datasets and determining what information we need to keep. Some of this cleaning will be done with the aid of the DataWrangler extension in VSCode. Each of the datasets seem to have only one value in the `Layer` column, but this maybe useful in combining with other datasets. Sediment, BioMass, WaterQuality and Indicator of Benthic Integrity all have \"B\" for Bottom or no value, while Taxonomic counts all has \"D,\" which is not in the userguide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions\n",
    "\n",
    "For each of the datasets, we will read in the CSV, replace and empty values and NaN with `None`.  We also settle on a consistent naming scheme for reported parameter, value, and units. For taxonomic data, this will largely be handled with later cleaning.\n",
    "\n",
    "For the`Sediment and WaterQuality datasets, we will create a dictionary between the parameter, its units, and its meaning. Since BioMass and Indicator of Benthic Integrity have 126 unique parameters and no corresponding units column, and the dictionaries must be hand coded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_intial_clean(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Rename Units to be more clear\n",
    "    rename_mapping = {\n",
    "        'ReportingParameter': 'ReportedParameter',\n",
    "        'IBIParameter': 'ReportedParameter',\n",
    "        'Parameter': 'ReportedParameter',\n",
    "        'ReportingValue': 'ReportedValue',\n",
    "        'IBIValue': 'ReportedValue',\n",
    "        'MeasureValue': 'ReportedValue',\n",
    "        'ReportingUnits': 'ReportedUnits',\n",
    "        'Unit': 'ReportedUnits',\n",
    "        'Units': 'SampleVolumeUnits',\n",
    "        'Project': 'ProjectIdentifier',\n",
    "        'Depth': 'SampleDepth',\n",
    "        'TotalDepth': 'StationDepth'\n",
    "    }\n",
    "\n",
    "    df = df.rename(columns={col: rename_mapping[col] for col in df.columns if col in rename_mapping})\n",
    "        \n",
    "    df_clean = df.replace('', np.nan).where(df.notna(), None)\n",
    "    return df_clean\n",
    "\n",
    "def param_dict(df, meanings_list):\n",
    "    parameters = df['ReportedParameter'].unique()\n",
    "\n",
    "    # Meanings list must be carefully created from the userguide\n",
    "    # Create a mapping from parameters to their meanings\n",
    "    param_meaning = {param: meaning for param, meaning in zip(parameters, meanings_list)}\n",
    "\n",
    "    # Create the initial dictionary with units and empty types\n",
    "    param_dict = {param: {'Meaning': \"\", 'Units': unit} for param, unit in zip(df['ReportedParameter'], df['ReportedUnits'])}\n",
    "\n",
    "    # Update the dictionary with the meanings\n",
    "    for param in param_dict:\n",
    "        if param in param_meaning:\n",
    "            param_dict[param]['Meaning'] = param_meaning[param]\n",
    "\n",
    "    return param_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want a function to check if two columns encode the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_column_values(df,column1,column2):\n",
    "    # Group by column1 and check the unique values in column2\n",
    "    groups = df.groupby(column1)[column2].nunique()\n",
    "\n",
    "    # Identify column1 values where LifeStageDescriptions has more than one unique value\n",
    "    disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df[column1].isin(disagreeing_a_values)]\n",
    "\n",
    "    filtered_df[[column1,column2]].drop_duplicates().sort_values(by=column1)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidal Benthic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sediment and water quality cleaning\n",
    "\n",
    "The sediment and water quality datasets have similar structures. It appears these are actually the same datasets. We will create dictionaries for both, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_file = '../../data/plankton-patrol_data/plank_ChesapeakeBenthicSediment.csv'\n",
    "waterTidalBenthic_file = '../../data/plankton-patrol_data/plank_ChesapeakeBenthicWaterQuality.csv'\n",
    "\n",
    "sediment = read_and_intial_clean(sediment_file)\n",
    "waterTidalBenthic = read_and_intial_clean(waterTidalBenthic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sediment parameters:  ['MOIST' 'SAND' 'TC' 'TIC' 'SILTCLAY' 'TOC' 'TN' 'VOLORG' 'KURTOSIS'\n",
      " 'CLAY' 'MEANDIAM' 'SORT' 'SKEWNESS']\n",
      "WaterQuality parameters:  ['PH' 'WTEMP' 'DO' 'DO_SAT_P' 'SALINITY' 'SPCOND']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sediment parameters: \", sediment['ReportedParameter'].unique())\n",
    "print(\"WaterQuality parameters: \", waterTidalBenthic['ReportedParameter'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since three of the parameters in the sediment dataset are statistical measures (kurtosis, skewness, sort), we need to remove these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sediment parameters:  ['MOIST' 'SAND' 'TC' 'TIC' 'SILTCLAY' 'TOC' 'TN' 'VOLORG' 'CLAY'\n",
      " 'MEANDIAM']\n"
     ]
    }
   ],
   "source": [
    "# List of parameters to remove\n",
    "parameters_to_remove = ['KURTOSIS', 'SKEWNESS', 'SORT']\n",
    "\n",
    "# Remove rows where 'ReportedParameter' is in the list of parameters to remove\n",
    "sediment = sediment[~sediment['ReportedParameter'].isin(parameters_to_remove)]\n",
    "\n",
    "print(\"Sediment parameters: \", sediment['ReportedParameter'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consulting [2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf) and keeping the information in the same order, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_parameters_meanings = ['Sediment Moisture Percentage', 'Sand Content, Percent', 'Total Carbon Content','Total Inorganic Carbonate Content','Silt Clay Content, Percent','Total Organic Carbon','Total Nitrogen','Volatile Organic, Percent','Clay Content, Percent','Mean Sediment Diameter']\n",
    "\n",
    "waterTidalBenthic_parameters_meanings = ['pH', 'Water Temperature', 'Dissolved Oxygen', 'Dissolved oxygen relative to theoretical value at saturation', 'Salinity', 'Specific Conductance At 25 C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_param_dict = param_dict(sediment,sediment_parameters_meanings)\n",
    "waterTidalBenthic_param_dict = param_dict(waterTidalBenthic,waterTidalBenthic_parameters_meanings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dictionary, we can drop the `ReportedUnits` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment = sediment.drop(columns='ReportedUnits')\n",
    "waterTidalBenthic = waterTidalBenthic.drop(columns='ReportedUnits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in sediment and water quality:  ['CBSeg2003', 'StationDepth', 'FieldActivityId', 'SampleTime', 'Salzone', 'Source', 'ProjectIdentifier', 'SampleVolume', 'Layer', 'SampleReplicate', 'Latitude', 'CBSeg2003Description', 'EventId', 'SampleVolumeUnits', 'Station', 'Longitude', 'ReportedValue', 'SampleDate', 'ReportedParameter', 'PDepth']\n",
      "Columns only in Sediment:  []\n",
      "Columns only in Water Quality:  ['WQMethod', 'SampleDepth']\n"
     ]
    }
   ],
   "source": [
    "sediment_columns = sediment.columns\n",
    "waterTidalBenthic_columns = waterTidalBenthic.columns\n",
    "\n",
    "# Find common columns\n",
    "sediment_and_water_columns = list(set(sediment_columns) & set(waterTidalBenthic_columns))\n",
    "\n",
    "# Find columns only in biomass\n",
    "columns_only_in_sediment = list(set(sediment_columns) - set(waterTidalBenthic_columns))\n",
    "\n",
    "# Find columns only in ibi\n",
    "columns_only_in_water = list(set(waterTidalBenthic_columns) - set(sediment_columns))\n",
    "\n",
    "print(\"Columns in sediment and water quality: \", sediment_and_water_columns)\n",
    "print(\"Columns only in Sediment: \", columns_only_in_sediment)\n",
    "print(\"Columns only in Water Quality: \", columns_only_in_water)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since  `WQMethod` is about the method used to collect the sample, we can drop it. We also need to drop `EventId` to prevent issues merging (since it also track the parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment = sediment.drop(columns=['EventId'])\n",
    "waterTidalBenthic = waterTidalBenthic.drop(columns=['WQMethod','EventId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioMass and Indicator of Benthic Integrity cleaning\n",
    "\n",
    "Since the BioMass and Indicator of Benthic Integrity datasets have similar structures, we will handle them together. We will then check if there are any differences between the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_file = '../../data/plankton-patrol_data/plank_ChesapeakeBenthicBioMass.csv'\n",
    "ibi_file = '../../data/plankton-patrol_data/plank_ChesapeakeBenthicIBI.csv'\n",
    "\n",
    "\n",
    "biomass = read_and_intial_clean(biomass_file)\n",
    "ibi = read_and_intial_clean(ibi_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows only in biomass:\n",
      "Empty DataFrame\n",
      "Columns: [CBSeg2003, CBSeg2003Description, Station, Latitude, Longitude, FieldActivityId, SampleDate, SampleTime, Layer, StationDepth, BiologicalEventId, Source, SampleReplicate, ReportedParameter, ReportedValue, ProjectIdentifier, SampleVolumeUnits, SampleVolume, PDepth, Salzone]\n",
      "Index: []\n",
      "Rows only in ibi:\n",
      "Empty DataFrame\n",
      "Columns: [CBSeg2003, CBSeg2003Description, Station, Latitude, Longitude, FieldActivityId, SampleDate, SampleTime, Layer, StationDepth, BiologicalEventId, Source, SampleReplicate, ReportedParameter, ReportedValue, ProjectIdentifier, SampleVolumeUnits, SampleVolume, PDepth, Salzone]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Merge with indicator\n",
    "merged_df = biomass.merge(ibi, how='outer', indicator=True)\n",
    "\n",
    "# Rows only in biomass\n",
    "only_in_biomass = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Rows only in ibi\n",
    "only_in_ibi = merged_df[merged_df['_merge'] == 'right_only'].drop(columns=['_merge'])\n",
    "\n",
    "print(\"Rows only in biomass:\")\n",
    "print(only_in_biomass)\n",
    "\n",
    "print(\"Rows only in ibi:\")\n",
    "print(only_in_ibi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are the same dataset, we will only work with biomass. Let's at least find out how many parameters we have and what columns we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioMass paramenters:  126\n",
      "Biomass columns:  Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'FieldActivityId', 'SampleDate', 'SampleTime', 'Layer', 'StationDepth',\n",
      "       'BiologicalEventId', 'Source', 'SampleReplicate', 'ReportedParameter',\n",
      "       'ReportedValue', 'ProjectIdentifier', 'SampleVolumeUnits',\n",
      "       'SampleVolume', 'PDepth', 'Salzone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "biomass_parameters = biomass['ReportedParameter'].unique()\n",
    "biomass_columns = biomass.columns\n",
    "\n",
    "\n",
    "print(\"BioMass paramenters: \", len(biomass_parameters))\n",
    "print(\"Biomass columns: \", biomass_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, are `BiologicalEventId` and `FieldActivityId` encoding the same information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSeg2003</th>\n",
       "      <th>CBSeg2003Description</th>\n",
       "      <th>Station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>FieldActivityId</th>\n",
       "      <th>SampleDate</th>\n",
       "      <th>SampleTime</th>\n",
       "      <th>Layer</th>\n",
       "      <th>StationDepth</th>\n",
       "      <th>BiologicalEventId</th>\n",
       "      <th>Source</th>\n",
       "      <th>SampleReplicate</th>\n",
       "      <th>ReportedParameter</th>\n",
       "      <th>ReportedValue</th>\n",
       "      <th>ProjectIdentifier</th>\n",
       "      <th>SampleVolumeUnits</th>\n",
       "      <th>SampleVolume</th>\n",
       "      <th>PDepth</th>\n",
       "      <th>Salzone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10555</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4193</td>\n",
       "      <td>-76.41889</td>\n",
       "      <td>218369</td>\n",
       "      <td>9/19/2006</td>\n",
       "      <td>13:40:00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>70216</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S1</td>\n",
       "      <td>GRAND_SCORE</td>\n",
       "      <td>1.56</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4193</td>\n",
       "      <td>-76.41889</td>\n",
       "      <td>218369</td>\n",
       "      <td>9/19/2006</td>\n",
       "      <td>13:40:00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>70216</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S1</td>\n",
       "      <td>PCT_CARN_OMN</td>\n",
       "      <td>6.90</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10557</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4193</td>\n",
       "      <td>-76.41889</td>\n",
       "      <td>218369</td>\n",
       "      <td>9/19/2006</td>\n",
       "      <td>13:40:00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>70216</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S1</td>\n",
       "      <td>PCT_CARN_OMN</td>\n",
       "      <td>11.76</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10558</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4193</td>\n",
       "      <td>-76.41889</td>\n",
       "      <td>218369</td>\n",
       "      <td>9/19/2006</td>\n",
       "      <td>13:40:00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>70216</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S1</td>\n",
       "      <td>PCT_CARN_OMN</td>\n",
       "      <td>15.38</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10559</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4193</td>\n",
       "      <td>-76.41889</td>\n",
       "      <td>218369</td>\n",
       "      <td>9/19/2006</td>\n",
       "      <td>13:40:00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.9</td>\n",
       "      <td>70216</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S1</td>\n",
       "      <td>PCT_PI_ABUND</td>\n",
       "      <td>41.18</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20802</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4191</td>\n",
       "      <td>-76.41850</td>\n",
       "      <td>224939</td>\n",
       "      <td>9/13/2012</td>\n",
       "      <td>12:18:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2.2</td>\n",
       "      <td>73162</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S2</td>\n",
       "      <td>TRICHOPTERA_TAXA_CNT</td>\n",
       "      <td>3.00</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20803</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4191</td>\n",
       "      <td>-76.41850</td>\n",
       "      <td>224939</td>\n",
       "      <td>9/13/2012</td>\n",
       "      <td>12:18:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2.2</td>\n",
       "      <td>73162</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S2</td>\n",
       "      <td>TRICHOPTERA_TAXA_CNT_R</td>\n",
       "      <td>3.00</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20804</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4191</td>\n",
       "      <td>-76.41850</td>\n",
       "      <td>224939</td>\n",
       "      <td>9/13/2012</td>\n",
       "      <td>12:18:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2.2</td>\n",
       "      <td>73162</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S2</td>\n",
       "      <td>TRICHOPTERA_TAXA_COUNT_NO_HYDR</td>\n",
       "      <td>2.00</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20805</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4191</td>\n",
       "      <td>-76.41850</td>\n",
       "      <td>224939</td>\n",
       "      <td>9/13/2012</td>\n",
       "      <td>12:18:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2.2</td>\n",
       "      <td>73162</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S2</td>\n",
       "      <td>TRICHOPTERA_TAXA_COUNT_NO_HYDR_R</td>\n",
       "      <td>2.00</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20806</th>\n",
       "      <td>CB4MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>001</td>\n",
       "      <td>38.4191</td>\n",
       "      <td>-76.41850</td>\n",
       "      <td>224939</td>\n",
       "      <td>9/13/2012</td>\n",
       "      <td>12:18:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2.2</td>\n",
       "      <td>73162</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>S2</td>\n",
       "      <td>TRICHOPTERA_TAXA_COUNT_NO_R</td>\n",
       "      <td>0.00</td>\n",
       "      <td>BEN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CBSeg2003              CBSeg2003Description Station  Latitude  \\\n",
       "10555    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4193   \n",
       "10556    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4193   \n",
       "10557    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4193   \n",
       "10558    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4193   \n",
       "10559    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4193   \n",
       "...         ...                               ...     ...       ...   \n",
       "20802    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4191   \n",
       "20803    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4191   \n",
       "20804    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4191   \n",
       "20805    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4191   \n",
       "20806    CB4MH   Chesapeake Bay-Mesohaline Region     001   38.4191   \n",
       "\n",
       "       Longitude  FieldActivityId SampleDate SampleTime Layer  StationDepth  \\\n",
       "10555  -76.41889           218369  9/19/2006   13:40:00  None           1.9   \n",
       "10556  -76.41889           218369  9/19/2006   13:40:00  None           1.9   \n",
       "10557  -76.41889           218369  9/19/2006   13:40:00  None           1.9   \n",
       "10558  -76.41889           218369  9/19/2006   13:40:00  None           1.9   \n",
       "10559  -76.41889           218369  9/19/2006   13:40:00  None           1.9   \n",
       "...          ...              ...        ...        ...   ...           ...   \n",
       "20802  -76.41850           224939  9/13/2012   12:18:00  None           2.2   \n",
       "20803  -76.41850           224939  9/13/2012   12:18:00  None           2.2   \n",
       "20804  -76.41850           224939  9/13/2012   12:18:00  None           2.2   \n",
       "20805  -76.41850           224939  9/13/2012   12:18:00  None           2.2   \n",
       "20806  -76.41850           224939  9/13/2012   12:18:00  None           2.2   \n",
       "\n",
       "       BiologicalEventId          Source SampleReplicate  \\\n",
       "10555              70216  VERSAR/EME/BEL              S1   \n",
       "10556              70216  VERSAR/EME/BEL              S1   \n",
       "10557              70216  VERSAR/EME/BEL              S1   \n",
       "10558              70216  VERSAR/EME/BEL              S1   \n",
       "10559              70216  VERSAR/EME/BEL              S1   \n",
       "...                  ...             ...             ...   \n",
       "20802              73162  VERSAR/EME/BEL              S2   \n",
       "20803              73162  VERSAR/EME/BEL              S2   \n",
       "20804              73162  VERSAR/EME/BEL              S2   \n",
       "20805              73162  VERSAR/EME/BEL              S2   \n",
       "20806              73162  VERSAR/EME/BEL              S2   \n",
       "\n",
       "                      ReportedParameter  ReportedValue ProjectIdentifier  \\\n",
       "10555                       GRAND_SCORE           1.56               BEN   \n",
       "10556                      PCT_CARN_OMN           6.90               BEN   \n",
       "10557                      PCT_CARN_OMN          11.76               BEN   \n",
       "10558                      PCT_CARN_OMN          15.38               BEN   \n",
       "10559                      PCT_PI_ABUND          41.18               BEN   \n",
       "...                                 ...            ...               ...   \n",
       "20802              TRICHOPTERA_TAXA_CNT           3.00               BEN   \n",
       "20803            TRICHOPTERA_TAXA_CNT_R           3.00               BEN   \n",
       "20804    TRICHOPTERA_TAXA_COUNT_NO_HYDR           2.00               BEN   \n",
       "20805  TRICHOPTERA_TAXA_COUNT_NO_HYDR_R           2.00               BEN   \n",
       "20806       TRICHOPTERA_TAXA_COUNT_NO_R           0.00               BEN   \n",
       "\n",
       "      SampleVolumeUnits  SampleVolume  PDepth Salzone  \n",
       "10555              None           NaN     NaN    None  \n",
       "10556              None           NaN     NaN    None  \n",
       "10557              None           NaN     NaN    None  \n",
       "10558              None           NaN     NaN    None  \n",
       "10559              None           NaN     NaN    None  \n",
       "...                 ...           ...     ...     ...  \n",
       "20802              None           NaN     NaN    None  \n",
       "20803              None           NaN     NaN    None  \n",
       "20804              None           NaN     NaN    None  \n",
       "20805              None           NaN     NaN    None  \n",
       "20806              None           NaN     NaN    None  \n",
       "\n",
       "[400 rows x 20 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_column_values(biomass,'FieldActivityId','BiologicalEventId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to drop `BiologicalEventId` to prevent issues merging (since it also track the parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass = biomass.drop(columns='BiologicalEventId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomic Counts\n",
    "\n",
    "The taxonomic counts dataset is structured fairly differently than the others. This is partly due to encoding taxonomic data, although not entirely. For example, there are two for encoding taxonomic information; one is numeric and the other is Latin name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_file = '../../data/plankton-patrol_data/plank_ChesapeakeBenthicTaxonomic.csv'\n",
    "\n",
    "taxonomic = read_and_intial_clean(taxonomic_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary for `LatinName` and the corresponding `TSN` number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_names_dict = {param: meaning for param, meaning in zip(taxonomic['TSN'], taxonomic['LatinName'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a dictionary for `LifeStageDescription`, from the table in the user guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89.,  79., 248.,  76.,  97., 247., 245.,  53.,  21., 225.,  nan,\n",
       "        93.,  52., 232.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomic['LifeStageDescription'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_stage_dict = {\n",
    "    89.: 'Not Specified',  \n",
    "    79.: 'Species', \n",
    "    248.: 'Immature Without Cap. Chaete',  \n",
    "    76.: 'Group',  \n",
    "    97.: 'Larvae', \n",
    "    247.: 'Immature With Cap. Chaete', \n",
    "    245.: 'Type',  \n",
    "    53.: 'Species B',  \n",
    "    21.: 'Pupae', \n",
    "    225.: 'Complex',  \n",
    "    93.: 'Juvenile',\n",
    "    52.: 'Species A', \n",
    "    232.: 'Species M'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is not clear how best to combine the numeric informatin in `TSN` and `LifeStageDescription` (life stages as a decimal, maybe?), we will combine the `LatinName` and `LifestageDescription`. Let's replace the `LifeStateDescription` with the actual description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic['LifeStageDescription'] = taxonomic['LifeStageDescription'].replace(life_stage_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many `LatinNames` are measured at multiple life stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LatinName</th>\n",
       "      <th>LifeStageDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24636</th>\n",
       "      <td>Ampelisca</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>Ampelisca</td>\n",
       "      <td>Juvenile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Axarus</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>Axarus</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11432</th>\n",
       "      <td>Bivalvia</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11076</th>\n",
       "      <td>Bivalvia</td>\n",
       "      <td>Species B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Chironomidae</td>\n",
       "      <td>Larvae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Chironomidae</td>\n",
       "      <td>Pupae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Chironomidae</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25684</th>\n",
       "      <td>Enchytraeidae</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11332</th>\n",
       "      <td>Enchytraeidae</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>Gastropoda</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3403</th>\n",
       "      <td>Gastropoda</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25546</th>\n",
       "      <td>Maldanidae</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9470</th>\n",
       "      <td>Maldanidae</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723</th>\n",
       "      <td>Nemertea</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Nemertea</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15988</th>\n",
       "      <td>Nereidae</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12892</th>\n",
       "      <td>Nereidae</td>\n",
       "      <td>Species A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Oligochaeta</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13813</th>\n",
       "      <td>Oligochaeta</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15754</th>\n",
       "      <td>Oligochaeta</td>\n",
       "      <td>Species M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Trepaxonemata</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11664</th>\n",
       "      <td>Trepaxonemata</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tubificidae</td>\n",
       "      <td>Immature Without Cap. Chaete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Tubificidae</td>\n",
       "      <td>Immature With Cap. Chaete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Tubificoides</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>Tubificoides</td>\n",
       "      <td>Group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LatinName          LifeStageDescription\n",
       "24636      Ampelisca                       Species\n",
       "8691       Ampelisca                      Juvenile\n",
       "46            Axarus                 Not Specified\n",
       "641           Axarus                       Species\n",
       "11432       Bivalvia                       Species\n",
       "11076       Bivalvia                     Species B\n",
       "33      Chironomidae                        Larvae\n",
       "351     Chironomidae                         Pupae\n",
       "629     Chironomidae                 Not Specified\n",
       "25684  Enchytraeidae                 Not Specified\n",
       "11332  Enchytraeidae                       Species\n",
       "9884      Gastropoda                       Species\n",
       "3403      Gastropoda                 Not Specified\n",
       "25546     Maldanidae                 Not Specified\n",
       "9470      Maldanidae                       Species\n",
       "8723        Nemertea                       Species\n",
       "41          Nemertea                 Not Specified\n",
       "15988       Nereidae                       Species\n",
       "12892       Nereidae                     Species A\n",
       "22       Oligochaeta                 Not Specified\n",
       "13813    Oligochaeta                       Species\n",
       "15754    Oligochaeta                     Species M\n",
       "192    Trepaxonemata                 Not Specified\n",
       "11664  Trepaxonemata                       Species\n",
       "3        Tubificidae  Immature Without Cap. Chaete\n",
       "168      Tubificidae     Immature With Cap. Chaete\n",
       "312     Tubificoides                       Species\n",
       "8701    Tubificoides                         Group"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by LatinName and check the unique values in LifeStageDescriptions\n",
    "groups = taxonomic.groupby('LatinName')['LifeStageDescription'].nunique()\n",
    "\n",
    "# Identify LatinName values where LifeStageDescriptions has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic[taxonomic['LatinName'].isin(disagreeing_a_values)]\n",
    "\n",
    "filtered_df[['LatinName','LifeStageDescription']].drop_duplicates().sort_values(by='LatinName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the `LatinName` and `LifeStageDescription` columns to create the `ReportedParameter` column. We will drop `Not Specified`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic = taxonomic.copy()\n",
    "\n",
    "taxonomic['ReportedParameter'] = taxonomic['LatinName'] + ' ' +taxonomic['LifeStageDescription'].replace('Not Specified', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is `TSN` always the same for `LatinName`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.drop_duplicates of Empty DataFrame\n",
       "Columns: [CBSeg2003, CBSeg2003Description, Station, Latitude, Longitude, SampleType, FieldActivityId, SampleDate, SampleTime, Layer, StationDepth, ReportedValue, ReportingUnit, EventId, Source, GMethod, TSN, LifeStageDescription, LatinName, ProjectIdentifier, SampleVolumeUnits, Salzone, PDepth, SampleVolume, ReportedParameter]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 25 columns]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by LatinName and check the unique values in TSN\n",
    "groups = taxonomic.groupby('LatinName')['TSN'].nunique()\n",
    "\n",
    "# Identify LatinName values where EventId has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "taxonomic[taxonomic['TSN'].isin(disagreeing_a_values)].drop_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be safe, we will keep the `TSN` column and drop the `LatinName` column, since `TSN` is already a numeric value. We will keep the `ReportedParameter` column and the `LifeStage` column, also to be safe.\n",
    "\n",
    "Also, for some reason every `ReportedUnit` is CM. The fact that there are many rows that only differ by the reported value suggest some are counts (and some are something else?). These values also vary a lot in scale, so the unit seems to not actually be consistent. We will drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_clean = taxonomic.drop(columns=['LatinName','ReportingUnit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomic columns:  Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'SampleType', 'FieldActivityId', 'SampleDate', 'SampleTime', 'Layer',\n",
      "       'StationDepth', 'ReportedValue', 'EventId', 'Source', 'GMethod', 'TSN',\n",
      "       'LifeStageDescription', 'ProjectIdentifier', 'SampleVolumeUnits',\n",
      "       'Salzone', 'PDepth', 'SampleVolume', 'ReportedParameter'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "taxonomic_columns = taxonomic_clean.columns\n",
    "print(\"Taxonomic columns: \",taxonomic_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with ReportedValues column -- not used later\n",
    "\n",
    "Since some values in the `ReportedValue` column appear to be counts and some are size, let's see if there are any rows that agree in all columns except `ReportedValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two entries=:  (20484, 23)\n",
      "Three entries=:  (1182, 23)\n",
      "More than three entries=:  (4090, 23)\n"
     ]
    }
   ],
   "source": [
    "# Columns to exclude from comparison\n",
    "exclude_columns = ['ReportedValue']\n",
    "\n",
    "# Columns to compare\n",
    "compare_columns = [col for col in taxonomic_clean.columns if col not in exclude_columns]\n",
    "\n",
    "# Group by the columns to compare\n",
    "grouped = taxonomic_clean.groupby(compare_columns)\n",
    "\n",
    "# Initialize a dictionary to collect rows based on group size\n",
    "group_dict = {\n",
    "    2: [],\n",
    "    3: [],\n",
    "    'more_than_3': []\n",
    "}\n",
    "\n",
    "# Iterate through each group\n",
    "for group_key, group in grouped:\n",
    "    num_rows = len(group)\n",
    "    \n",
    "    # Separate groups based on number of rows\n",
    "    if num_rows == 2:\n",
    "        group_dict[2].append(group)\n",
    "    elif num_rows == 3:\n",
    "        group_dict[3].append(group)\n",
    "    elif num_rows > 3:\n",
    "        group_dict['more_than_3'].append(group)\n",
    "\n",
    "# Function to create DataFrame from groups\n",
    "def create_dataframe(groups, sort_by):\n",
    "    return pd.concat(groups).sort_values(by=sort_by) if groups else pd.DataFrame()\n",
    "\n",
    "# Create DataFrames for viewing\n",
    "df_groups_with_2_rows = create_dataframe(group_dict[2], ['ReportedParameter', 'EventId'])\n",
    "df_groups_with_3_rows = create_dataframe(group_dict[3], ['ReportedParameter', 'EventId', 'ReportedValue'])\n",
    "df_groups_with_more_than_3_rows = create_dataframe(group_dict['more_than_3'], ['ReportedParameter', 'EventId', 'ReportedValue']).drop_duplicates()\n",
    "\n",
    "# Print shapes\n",
    "print(\"Two entries=: \",df_groups_with_2_rows.shape)\n",
    "print(\"Three entries=: \",df_groups_with_3_rows.shape)\n",
    "print(\"More than three entries=: \",df_groups_with_more_than_3_rows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Quality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/kqt6tbv90l9_pwc4927vdb340000gn/T/ipykernel_1608/840859314.py:2: DtypeWarning: Columns (2,24,25,29,31,32,33,34,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "water_file = '../../data/plank_ChesapeakeWaterQuality.csv'\n",
    "water = read_and_intial_clean(water_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the DtypeWarning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BiasPC', 'FieldActivityEventType', 'PrecipType'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water.columns[[24, 29, 33]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BiasPC` and `FieldActivityEventType` are almost entirely missing, so we can drop those. Let's look at the unique entries in `PrecipType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, 'Drizzle', 'Rain', 'Squally', 'Heavy Rain',\n",
       "       'Frozen Precipitation'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water['PrecipType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = water.drop(columns=['BiasPC','FieldActivityEventType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do some further renaming to match with the Tidal Bethic datasets. Since `EventId` is used differently in the two datasets, let's rename this on `FieldActivityId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = water.rename(columns={'EventId': 'FieldActivityId'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CBSeg2003', 'FieldActivityId', 'Cruise', 'Program',\n",
       "       'ProjectIdentifier', 'Agency', 'Source', 'Station', 'SampleDate',\n",
       "       'SampleTime', 'StationDepth', 'UpperPycnocline', 'LowerPycnocline',\n",
       "       'SampleDepth', 'Layer', 'SampleType', 'SampleReplicateType',\n",
       "       'ReportedParameter', 'Qualifier', 'ReportedValue', 'ReportedUnits',\n",
       "       'Method', 'Lab', 'Problem', 'Details', 'Latitude', 'Longitude',\n",
       "       'TierLevel', 'AirTemp', 'WindSpeed', 'WindDirection', 'PrecipType',\n",
       "       'TideStage', 'WaveHeight', 'CloudCover'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also remove the columns that correspond to how the data was collected or measured (`Program`, `Agency`, `Method`, `Lab`, `TierLevel`). We also remove `SampleReplicateType`, as this could cause errors with merging, with different parts of the sample being in different replicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = water.drop(columns=['Program', 'Agency', 'Method', 'Lab','TierLevel','SampleReplicateType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our parameter dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water parameters:  ['CHLA' 'DIN' 'DO' 'DON' 'DOP' 'KD' 'NH4F' 'NO23F' 'NO2F' 'NO3F' 'PC' 'PH'\n",
      " 'PHEO' 'PN' 'PO4F' 'PP' 'SALINITY' 'SECCHI' 'SIF' 'SIGMA_T' 'SPCOND'\n",
      " 'TDN' 'TDP' 'TN' 'TON' 'TP' 'TSS' 'WTEMP' 'VSS' 'DOC' 'PIP' 'FSS'\n",
      " 'TURB_NTU' 'DO_SAT_P']\n"
     ]
    }
   ],
   "source": [
    "print(\"Water parameters: \", water['ReportedParameter'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial dictionary with parameter and its meaning\n",
    "water_param_meaning = ['Chlorophyll-a', 'Dissolved inorganic nitrogen', 'Dissolved oxygen', \n",
    "                    'Dissolved organic nitrogen', \n",
    "                    'Dissolved organic phosphorus',\n",
    "                    'Light attenuation',\n",
    "                    'Ammonium (filtered)',\n",
    "                    'Nitrite + nitrate (filtered)',\n",
    "                    'Nitrite ( filtered)', \n",
    "                    'Nitrite ( filtered)',\n",
    "                    'Particulate organic carbon',\n",
    "                    'pH corrected for temperature 25 C',\n",
    "                    'pheophytin',\n",
    "                    'Particulate Organic Nitrogen and Particulate Nitrogen', \n",
    "                    'Orthophosphorus (filtered)', \n",
    "                    'Particulate phosphorus',\n",
    "                    'Salinity', \n",
    "                    'Secchi disk depth',\n",
    "                    'Silica (filtered)', \n",
    "                    'Water density',\n",
    "                    'Specific conductance At 25 C',\n",
    "                    'Total dissolved nitrogen',\n",
    "                    'Total dissolved phosphorus', \n",
    "                    'Total nitrogen',\n",
    "                    'Total organic nitrogen',\n",
    "                    'Total phosphorus',\n",
    "                    'Total suspended solids', \n",
    "                    'Water temperature',\n",
    "                    'Volitile Suspended Solids',\n",
    "                    'Dissolved organic carbon',\n",
    "                    'None',\n",
    "                    'Particulate inorganic phosphorus',\n",
    "                    'Fixed suspended solids',\n",
    "                    'Turbidity: nephelometric method',\n",
    "                    'Dissolved oxygen saturation using probe units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_param_dict = param_dict(water,water_param_meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove the units column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = water.drop(columns='ReportedUnits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also remove the `Qualifier` column, for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "water  = water.drop(columns='Qualifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn parameters into columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common functions\n",
    "\n",
    "First, let's see what columns are common to all for cleaned dataframes and which are unique. Sice we will want to run this step a few times, we will define a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_to_columns(dataframe,columns_to_group):\n",
    "    print(\"Original dataframe shape: \", dataframe.shape)\n",
    "    # Reset index to use row numbers as the index\n",
    "    df_reset = dataframe.reset_index(drop=True)\n",
    "\n",
    "    # Pivot the DataFrame while preserving non-pivoted columns\n",
    "    df_pivot = df_reset.pivot_table(index=df_reset.index, columns='ReportedParameter', values='ReportedValue', aggfunc='first')\n",
    "\n",
    "    # Combine pivoted result with the original DataFrame columns not involved in the pivot\n",
    "    df_pivot_combined = df_pivot.join(df_reset,how='outer').drop(columns=['ReportedParameter', 'ReportedValue'])\n",
    "    \n",
    "    # Create a copy of the DataFrame for processing\n",
    "    df_processed = df_pivot_combined.copy()\n",
    "\n",
    "    # Create a unique identifier for each group based on the columns to match\n",
    "    df_processed['UniqueID'] = df_processed[columns_to_group].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "    # Group by the unique identifier\n",
    "    df_combined = df_processed.groupby('UniqueID', as_index=False).first()\n",
    "\n",
    "    # Drop the UniqueID column and remove duplicates\n",
    "    df_clean = df_combined.drop(columns='UniqueID').drop_duplicates()\n",
    "\n",
    "    print(\"New dataframe shape: \", df_clean.shape)\n",
    "\n",
    "    return df_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiadal Benthic\n",
    "\n",
    "Let's find which columns are in all four tidal benthic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = list(set(sediment_and_water_columns) & set(biomass_columns) & set(taxonomic_columns)-set(['ReportedParameter', 'ReportedValue']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe shape:  (4695, 19)\n",
      "New dataframe shape:  (893, 27)\n"
     ]
    }
   ],
   "source": [
    "sediment_clean = parameter_to_columns(sediment,common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe shape:  (8427, 20)\n",
      "New dataframe shape:  (892, 24)\n"
     ]
    }
   ],
   "source": [
    "waterTidalBenthic_clean = parameter_to_columns(waterTidalBenthic,common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe shape:  (26904, 19)\n",
      "New dataframe shape:  (854, 143)\n"
     ]
    }
   ],
   "source": [
    "biomass_clean = parameter_to_columns(biomass, common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe shape:  (26398, 25)\n",
      "New dataframe shape:  (826, 432)\n"
     ]
    }
   ],
   "source": [
    "taxonomic_clean = parameter_to_columns(taxonomic, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More dictionary creation\n",
    "Now we can do a bit more work with the dictionaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Biomass\n",
    "Let's find a good threshold for missing data vs number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters above 10%: 121\n",
      "Parameters: ['PCT_EPT_TAXA_RICH_R', 'PCT_DOM2', 'PCT_NET_CADDISFLY_R', 'PCT_DOM1', 'PCT_NET_CADDISFLY', 'PLECOPTERA_TAXA_CNT_R', 'ASPT_MOD_R', 'NCO_TAXA_CNT_R', 'PCT_EPHEMEROPTERA_R', 'EPT_TAXA_COUNT', 'PCT_CLINGER_TAXA_R', 'FSW_R', 'SENSITIVE_TAXA_COUNT', 'PCT_CLIMB_R', 'PCT_SIMULIIDAE_R', 'ASPT_MOD', 'TRICHOPTERA_TAXA_CNT', 'TOLERANT_TAXA_COUNT_R', 'PCT_DOM3_R', 'PCT_EPT_R', 'TAXA_RICH', 'PCT_SENSITIVE', 'TAXA_RICH_R', 'PCT_DOM1_R', 'TOT_BIOMASS_G', 'PCT_PS_ABUND', 'PCT_COLLECT_R', 'PCT_PLECOPTERA_R', 'PCT_GATHER_R', 'SIMPSON_DIVERSITY_R', 'MARGALEFS', 'TOTAL_ABUNDANCE', 'TOTAL_SCORE', 'DIPTERA_TAXA_CNT_R', 'PCT_EPHEMEROPTERA', 'EPT_TAXA_COUNT_NO_TOL', 'GOLD', 'PCT_CLING_R', 'SW', 'NCO_TAXA_CNT', 'PCT_FILTERERS_R', 'PCT_DEPO', 'NON_INSECT_TAXA_CNT_R', 'PCT_BURROWER', 'TRICHOPTERA_TAXA_CNT_R', 'PCT_GATHER', 'TOT_ABUND', 'EPHEMEROPTERA_TAXA_CNT_R', 'PCT_NON_INSECT', 'PCT_DIPTERA', 'PCT_PREDATOR', 'PCT_PLECOPTERA', 'FBI', 'EPT_TAXA_ABUND', 'TOLERANCE', 'PCT_CARN_OMN', 'PCT_PI_BIO', 'TRICHOPTERA_TAXA_COUNT_NO_HYDR', 'RATIO_SC_TO_SH_R', 'PCT_PI_ABUND', 'PCT_PS_BIO', 'PCT_TRICHOPTERA_R', 'EPT_TAXA_COUNT_NO_TOL_R', 'PCT_SCRAPER_R', 'NON_INSECT_TAXA_CNT', 'PLECOPTERA_TAXA_CNT', 'PCT_SENSITIVE_R', 'PCT_PI_O_ABUND', 'PCT_TRICHOPTERA_NO_TOL', 'SCRAPER_TAXA_CNT_R', 'BECK_R', 'PCT_PS_O_ABUND', 'PCT_SHREDDER_R', 'PCT_CLING', 'PCT_CLIMB', 'PCT_TRICHOPTERA_NO_TOL_R', 'DIPTERA_TAXA_CNT', 'RATIO_SC_TO_SH', 'RATIO_SH_TO_CG', 'EPT_TAXA_ABUND_R', 'PCT_FILTERERS', 'PCT_SWIMMER_R', 'PCT_SIMULIIDAE', 'GOLD_R', 'PCT_COLLECT', 'PCT_SCRAPER', 'PCT_CHIRONOMIDAE_R', 'PCT_SHREDDER', 'SCRAPER_TAXA_CNT', 'TOLERANT_TAXA_COUNT', 'PCT_DIPTERA_R', 'SENSITIVE_TAXA_COUNT_R', 'PCT_TANYPODINI', 'PCT_TOLERANT', 'EPT_TAXA_COUNT_R', 'RATIO_SH_TO_CG_R', 'PCT_NON_INSECT_R', 'PCT_PI_F_ABUND', 'SIMPSON_DIVERSITY', 'PCT_TRICHOPTERA', 'BECK', 'TRICHOPTERA_TAXA_COUNT_NO_R', 'PCT_EPT_TAXA_RICH', 'PCT_CLINGER_TAXA', 'PCT_SWIMMER', 'PCT_DOM2_R', 'PCT_LIMESTONE_R', 'PCT_LIMESTONE', 'RATIO_SC_TO_CF', 'FSW', 'PCT_PREDATOR_R', 'PCT_DOM3', 'TOTAL_ABUNDANCE_R', 'PCT_EPT', 'RATIO_SC_TO_CF_R', 'FBI_R', 'PCT_BURROWER_R', 'PCT_TOLERANT_R', 'MARGALEFS_R', 'PCT_CHIRONOMIDAE', 'EPHEMEROPTERA_TAXA_CNT']\n",
      "Number of parameters above 20%: 15\n",
      "Parameters: ['PCT_PI_ABUND', 'PCT_PS_BIO', 'TOT_BIOMASS_G', 'PCT_PS_ABUND', 'PCT_TANYPODINI', 'TOTAL_SCORE', 'PCT_PI_O_ABUND', 'PCT_PI_F_ABUND', 'SW', 'PCT_DEPO', 'PCT_PS_O_ABUND', 'TOT_ABUND', 'TOLERANCE', 'PCT_CARN_OMN', 'PCT_PI_BIO']\n",
      "Number of parameters above 30%: 15\n",
      "Parameters: ['PCT_PI_ABUND', 'PCT_PS_BIO', 'TOT_BIOMASS_G', 'PCT_PS_ABUND', 'PCT_TANYPODINI', 'TOTAL_SCORE', 'PCT_PI_O_ABUND', 'PCT_PI_F_ABUND', 'SW', 'PCT_DEPO', 'PCT_PS_O_ABUND', 'TOT_ABUND', 'TOLERANCE', 'PCT_CARN_OMN', 'PCT_PI_BIO']\n",
      "Number of parameters above 40%: 10\n",
      "Parameters: ['PCT_PI_ABUND', 'TOT_BIOMASS_G', 'PCT_PS_BIO', 'TOT_ABUND', 'PCT_PS_ABUND', 'PCT_DEPO', 'TOTAL_SCORE', 'PCT_CARN_OMN', 'SW', 'PCT_PI_BIO']\n",
      "Number of parameters above 50%: 10\n",
      "Parameters: ['PCT_PI_ABUND', 'TOT_BIOMASS_G', 'PCT_PS_BIO', 'TOT_ABUND', 'PCT_PS_ABUND', 'PCT_DEPO', 'TOTAL_SCORE', 'PCT_CARN_OMN', 'SW', 'PCT_PI_BIO']\n",
      "Number of parameters above 60%: 10\n",
      "Parameters: ['PCT_PI_ABUND', 'TOT_BIOMASS_G', 'PCT_PS_BIO', 'TOT_ABUND', 'PCT_PS_ABUND', 'PCT_DEPO', 'TOTAL_SCORE', 'PCT_CARN_OMN', 'SW', 'PCT_PI_BIO']\n",
      "Number of parameters above 70%: 8\n",
      "Parameters: ['TOT_BIOMASS_G', 'TOT_ABUND', 'PCT_PS_ABUND', 'PCT_DEPO', 'TOTAL_SCORE', 'PCT_CARN_OMN', 'SW', 'PCT_PI_BIO']\n",
      "Number of parameters above 80%: 6\n",
      "Parameters: ['TOT_BIOMASS_G', 'TOT_ABUND', 'PCT_PS_ABUND', 'TOTAL_SCORE', 'SW', 'PCT_PI_BIO']\n",
      "Number of parameters above 90%: 4\n",
      "Parameters: ['TOT_ABUND', 'SW', 'TOT_BIOMASS_G', 'TOTAL_SCORE']\n",
      "Number of parameters above 100%: 0\n",
      "Parameters: []\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store columns meeting each threshold\n",
    "columns_meeting_thresholds = {}\n",
    "\n",
    "# Iterate through 10% intervals from 10% to 100%\n",
    "for i in range(1, 11):\n",
    "    threshold_percent = i * 10\n",
    "    threshold = len(biomass_clean) * threshold_percent / 100\n",
    "\n",
    "    # Determine the columns that meet this threshold\n",
    "    columns_with_threshold_values = biomass_clean.columns[biomass_clean.count() >= threshold]\n",
    "\n",
    "    # Filter the DataFrame to include only these columns\n",
    "    filtered_df = biomass_clean[columns_with_threshold_values]\n",
    "\n",
    "    # Determine new parameters\n",
    "    biomass_new_parameters = list(set(filtered_df.columns) - set(common_columns) - set(['SampleReplicate', 'BiologicalEventId']))\n",
    "\n",
    "    # Store the results\n",
    "    columns_meeting_thresholds[f'{threshold_percent}%'] = biomass_new_parameters\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Number of parameters above {threshold_percent}%: {len(biomass_new_parameters)}\")\n",
    "    print(f\"Parameters: {biomass_new_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above 30% with 15 parameters looks reasonable. Let's work on creating a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PCT_PI_ABUND', 'PCT_PS_BIO', 'TOT_BIOMASS_G', 'PCT_PS_ABUND', 'PCT_TANYPODINI', 'TOTAL_SCORE', 'PCT_PI_O_ABUND', 'PCT_PI_F_ABUND', 'SW', 'PCT_DEPO', 'PCT_PS_O_ABUND', 'TOT_ABUND', 'TOLERANCE', 'PCT_CARN_OMN', 'PCT_PI_BIO']\n"
     ]
    }
   ],
   "source": [
    "threshold_percent = 30\n",
    "threshold = len(biomass_clean) * threshold_percent / 100\n",
    "\n",
    "columns_with_threshold_values = biomass_clean.columns[biomass_clean.count() >= threshold]\n",
    "\n",
    "filtered_df = biomass_clean[columns_with_threshold_values]\n",
    "\n",
    "# Determine new parameters\n",
    "biomass_new_parameters = list(set(filtered_df.columns) - set(common_columns) - set(['SampleReplicate', 'BiologicalEventId']))\n",
    "print(biomass_new_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the biomass datatset does not have a units column. This means we must create the dictionary another way. Most of these are percentages, which we can handle easily. `TOTAL_SCORE`, `TOLERANCE` and `SW` are calculated scores of benthic macroinvertebrate community structure and function, which do not require units. The remaining parameters, `TOT_ABUND` and `TOT_BIOMASS_G` are listed as \"units will vary\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_param_dict = {}\n",
    "for param in biomass_new_parameters:\n",
    "    if 'PCT' in param:\n",
    "        biomass_param_dict[param] = {'Meaning': \"\", 'Units': 'PCT'}\n",
    "    if 'TOT_' in param:\n",
    "        biomass_param_dict[param] = {'Meaning': \"\", 'Units': 'Will Vary'}\n",
    "    else:\n",
    "        biomass_param_dict[param] = {'Meaning': \"\", 'Units': \"None\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "biomass_param_meaning = {'PCT_PI_F_ABUND', 'PCT_DEPO', 'TOT_ABUND', 'PCT_CARN_OMN', 'TOT_BIOMASS_G', 'PCT_PI_O_ABUND', 'PCT_PI_ABUND', 'PCT_PS_ABUND', 'PCT_TANYPODINI', 'SW', 'TOTAL_SCORE', 'PCT_PS_O_ABUND', 'TOLERANCE', 'PCT_PS_BIO', 'PCT_PI_BIO'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_param_to_meaning = {\"TOTAL_SCORE\": \"Total Benthic Restoration Goal Score For Sample\",\n",
    "                            \"PCT_TANYPODINI\": \"Percent Tanypodinae To Chironomidae-Tidal Benthic\",\n",
    "                            \"PCT_PS_O_ABUND\": \"Percent Pollution Sensitive Species Abundance-Oligohaline Benthic\",\n",
    "                            \"PCT_PS_BIO\": \"Percent Pollution Sensitive Species Abundance-Tidal Fresh Benthic\",\n",
    "                            \"PCT_DEPO\": \"Percent Deep Deposit Feeders\",\n",
    "                            \"PCT_PI_F_ABUND\":  \"Percent Pollution Indicative Species Biomass-Tidal Benthic\",\n",
    "                            \"PCT_PI_BIO\": \"Percent Pollution Indicative Species Biomass-Tidal Benthic\",\n",
    "                            \"PCT_CARN_OMN\": \"Percent Carnivores And Omnivores\",\n",
    "                            \"TOT_BIOMASS_G\": \"Total Species Biomass\",\n",
    "                            \"PCT_PI_ABUND\": \"Percent Pollution Indicative Species Abundance-Tidal Benthic\",\n",
    "                            \"TOLERANCE\": \"Tidal Benthic Pollution Tolerance Index\",\n",
    "                            \"PCT_PS_ABUND\": \"Percent Pollution Sensitive Species Abundance-Oligohaline Benthic\",\n",
    "                            \"TOT_ABUND\": \"Total Number Of Individuals\",\n",
    "                            \"PCT_PI_O_ABUND\": \"Percent Pollution Indicative Species Biomass-Oligohaline Benthic\",\n",
    "                            \"SW\": \"Shannon Wiener Index\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the 'Meaning' field in biomass_param_dict\n",
    "for key in biomass_param_dict:\n",
    "    if key in biomass_param_to_meaning:\n",
    "        biomass_param_dict[key]['Meaning'] = biomass_param_to_meaning[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of these vales (SW : Shannon Wiener Index, TOTAL_SCORE : Total Benthic Restoration Goal Score For Sample, and \n",
    "TOLERANCE :Tidal Benthic Pollution Tolerance Index) are calculated scores of ecosystem health with no units. Samples with index values of 3.0 or more are\n",
    "considered to have good benthic condition indicative of good habitat quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taxonomic counts\n",
    "Let's find a good threshold for missing data vs number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
       "       'SampleType', 'FieldActivityId', 'SampleDate', 'SampleTime', 'Layer',\n",
       "       'StationDepth', 'ReportedValue', 'ReportingUnit', 'EventId', 'Source',\n",
       "       'GMethod', 'TSN', 'LifeStageDescription', 'LatinName',\n",
       "       'ProjectIdentifier', 'SampleVolumeUnits', 'Salzone', 'PDepth',\n",
       "       'SampleVolume', 'ReportedParameter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomic.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters above 10%: 46\n",
      "Parameters: ['Edotea triloba ', 'Micrura leidyi ', 'Macroclymene zonalis ', 'Carinoma tremaphoros ', 'Caulleriella killariensis ', 'Phoronis Species', 'Leptocheirus plumulosus ', 'Streblospio benedicti ', 'Rictaxis punctostriatus ', 'Ampelisca verrilli ', 'Mediomastus ambiseta ', 'Neanthes succinea ', 'Parvilucina multilineata ', 'Phyllodoce arenae ', 'Paraprionospio pinnata ', 'Tubificidae Immature Without Cap. Chaete', 'Tubificoides Species', 'Glycera americana ', 'Spiophanes bombyx ', 'Ameroculodes Complex', 'Loimia medusa ', 'Marenzelleria viridis ', 'Spiochaetopterus costarum ', 'Podarkeopsis levifuscina ', 'Glycera dibranchiata ', 'Mulinia lateralis ', 'Macoma baltica ', 'Gemma gemma ', 'Pectinaria gouldi ', 'Clymenella torquata ', 'Tellina agilis ', 'Cyathura polita ', 'Acteocina canaliculata ', 'Sigambra tentaculata ', 'Listriella barnardi ', 'Rangia cuneata ', 'Glycinde solitaria ', 'Nephtys picta ', 'Macoma mitchelli ', 'Heteromastus filiformis ', 'Stylochus ellipticus ', 'Leitoscoloplos Species', 'Ampelisca Juvenile', 'Prionospio perkinsi ', 'Eteone heteropoda ', 'Nemertea Species']\n",
      "Number of parameters above 20%: 17\n",
      "Parameters: ['Macoma mitchelli ', 'Heteromastus filiformis ', 'Paraprionospio pinnata ', 'Phoronis Species', 'Carinoma tremaphoros ', 'Tubificoides Species', 'Leptocheirus plumulosus ', 'Cyathura polita ', 'Streblospio benedicti ', 'Loimia medusa ', 'Acteocina canaliculata ', 'Marenzelleria viridis ', 'Spiochaetopterus costarum ', 'Nemertea Species', 'Glycinde solitaria ', 'Mediomastus ambiseta ', 'Neanthes succinea ']\n",
      "Number of parameters above 30%: 11\n",
      "Parameters: ['Heteromastus filiformis ', 'Paraprionospio pinnata ', 'Phoronis Species', 'Neanthes succinea ', 'Tubificoides Species', 'Streblospio benedicti ', 'Acteocina canaliculata ', 'Marenzelleria viridis ', 'Glycinde solitaria ', 'Macoma mitchelli ', 'Mediomastus ambiseta ']\n",
      "Number of parameters above 40%: 4\n",
      "Parameters: ['Heteromastus filiformis ', 'Glycinde solitaria ', 'Neanthes succinea ', 'Streblospio benedicti ']\n",
      "Number of parameters above 50%: 2\n",
      "Parameters: ['Glycinde solitaria ', 'Neanthes succinea ']\n",
      "Number of parameters above 60%: 0\n",
      "Parameters: []\n",
      "Number of parameters above 70%: 0\n",
      "Parameters: []\n",
      "Number of parameters above 80%: 0\n",
      "Parameters: []\n",
      "Number of parameters above 90%: 0\n",
      "Parameters: []\n",
      "Number of parameters above 100%: 0\n",
      "Parameters: []\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store columns meeting each threshold\n",
    "columns_meeting_thresholds = {}\n",
    "\n",
    "# Iterate through 10% intervals from 10% to 100%\n",
    "for i in range(1, 11):\n",
    "    threshold_percent = i * 10\n",
    "    threshold = len(taxonomic_clean) * threshold_percent / 100\n",
    "\n",
    "    # Determine the columns that meet this threshold\n",
    "    columns_with_threshold_values = taxonomic_clean.columns[taxonomic_clean.count() >= threshold]\n",
    "\n",
    "    # Filter the DataFrame to include only these columns\n",
    "    filtered_df = taxonomic_clean[columns_with_threshold_values]\n",
    "\n",
    "    # Determine new parameters\n",
    "    taxonomic_new_parameters = list(set(filtered_df.columns) - set(common_columns) - set(['GMethod', 'TSN', 'LifeStageDescription', 'LatinName', 'ProjectIdentifier','EventId', 'SampleType', 'ReportingUnit']))\n",
    "\n",
    "    # Store the results\n",
    "    columns_meeting_thresholds[f'{threshold_percent}%'] = taxonomic_new_parameters\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Number of parameters above {threshold_percent}%: {len(taxonomic_new_parameters)}\")\n",
    "    print(f\"Parameters: {taxonomic_new_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Quality database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing with water quality. There is only one (very large) csv. Let's get the columns to group by calling the API for one year of monitoring data. Some of the columns were empty, so we will intersect the monitor data columns with our water data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_data = pd.read_csv('https://datahub.chesapeakebay.net/api.CSV/WaterQuality/MonitorEvent/8-5-2023/8-6-2024/2,4,6/12,13,15,35,36,2,3,7,33,34,23,24/CBSeg2003/10,11,12,13,14,15,16,17,28,49,84')\n",
    "columns_to_group = monitor_data.columns.intersection(water.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe shape:  (2164289, 27)\n",
      "New dataframe shape:  (29236, 59)\n"
     ]
    }
   ],
   "source": [
    "water_clean = parameter_to_columns(water, columns_to_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19799"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_clean['FieldActivityId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_clean.to_csv('../../data/plankton-patrol_data/plank_ChesapeakeBayWater_pivoted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Tidal Benthic Datasets\n",
    "\n",
    "Now we combine on the common columns. The taxonomic data is so sparce, we will omit it. We will include all of the biomass data, as a lot of columns can probably be combined (ones differentiated by salinity, for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of some extra columns before merging. `BiologicalEventId` is only in BioMass, `EventId` is simply a different system for recoding the same information as `FieldActivityId`. `SampleReplicate` should not matter, but migh prevent some merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_clean = sediment_clean.drop(columns=['SampleReplicate'])\n",
    "waterTidalBenthic_clean = waterTidalBenthic_clean.drop(columns=['SampleReplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sediment_clean and biomass_clean\n",
    "tidalBenthic = pd.merge(sediment_clean, biomass_clean, how='outer', on=[col for col in sediment_clean.columns if col in biomass_clean.columns], suffixes=('', '_biomass_clean'))\n",
    "\n",
    "# Merge the result with waterTidalBenthic_clean\n",
    "tidalBenthic = pd.merge(tidalBenthic, waterTidalBenthic_clean, how='outer', on=[col for col in tidalBenthic.columns if col in waterTidalBenthic_clean.columns and not col.endswith(('_biomass_clean', '_taxonomic_clean'))], suffixes=('', '_water_clean'))\n",
    "\n",
    "# Reset the index for better readability\n",
    "tidalBenthic = tidalBenthic.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some issues with different precisions for latitude and longitude causing lack of matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define columns for matching\n",
    "match_columns = list(set(sediment_clean.columns) & set(waterTidalBenthic_clean.columns) & set(biomass_clean.columns))\n",
    "\n",
    "# Generate a composite key based on the matching columns\n",
    "tidalBenthic['unique_key'] = tidalBenthic[match_columns].apply(lambda row: tuple(row.fillna('missing')), axis=1)\n",
    "\n",
    "# Handle Latitude and Longitude with precision\n",
    "# Keep the most precise value for Latitude and Longitude\n",
    "tidalBenthic['Latitude'] = tidalBenthic.groupby('unique_key')['Latitude'].transform(lambda x: x.dropna().iloc[0] if not x.dropna().empty else np.nan)\n",
    "tidalBenthic['Longitude'] = tidalBenthic.groupby('unique_key')['Longitude'].transform(lambda x: x.dropna().iloc[0] if not x.dropna().empty else np.nan)\n",
    "\n",
    "# Aggregate the groups\n",
    "tidalBenthic_clean = tidalBenthic.groupby('unique_key').first().reset_index()\n",
    "\n",
    "# Drop the unique_key column from the result\n",
    "tidalBenthic_clean = tidalBenthic_clean.drop(columns=['unique_key'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidalBenthic_clean.to_csv('../../data/plankton-patrol_data/plank_ChesapeakeBayBenthic_pivoted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `FieldActivityId` mighht be coded differently, and really is used for combing rows from the same database, let's drop it from both. We probably won't need `CBSeg2003Description`, whihch is not in the water dataset. It can be added back with a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_extra_clean = water_clean.drop(columns='FieldActivityId')\n",
    "tidalBenthic_extra_clean = tidalBenthic_clean.drop(columns=['FieldActivityId','CBSeg2003Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tidalBenthic_extra and water_extra_clean\n",
    "CPBDathub = pd.merge(tidalBenthic_extra_clean, water_extra_clean, how='outer', on=[col for col in tidalBenthic_extra_clean.columns if col in water_extra_clean.columns], suffixes=('_tidal', '_water'))\n",
    "\n",
    "# Reset the index for better readability\n",
    "CPBDathub = CPBDathub.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPBDathub.to_csv('../../data/plankton-patrol_data/plank_ChesapeakeBay_clean.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPBDathub.to_csv('../../data/plankton-patrol_data/plank_ChesapeakeBayDataHubCombined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dictionaries\n",
    "\n",
    "Let's make sure the dictionaries can be used later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable name: _oh\n",
      "Variable name: Out\n",
      "Variable name: sediment_param_dict\n",
      "Variable name: waterTidalBenthic_param_dict\n",
      "Variable name: taxonomic_names_dict\n",
      "Variable name: life_stage_dict\n",
      "Variable name: group_dict\n",
      "Variable name: water_param_dict\n",
      "Variable name: columns_meeting_thresholds\n",
      "Variable name: biomass_param_dict\n",
      "Variable name: biomass_param_to_meaning\n",
      "Variable name: all_vars\n"
     ]
    }
   ],
   "source": [
    "# Find them!\n",
    "\n",
    "# Get all variables in the current notebook's namespace\n",
    "all_vars = globals()\n",
    "\n",
    "# Filter variables to find dictionaries\n",
    "dict_vars = {name: obj for name, obj in all_vars.items() if isinstance(obj, dict)}\n",
    "\n",
    "# Print dictionary variables\n",
    "for name, dictionary in dict_vars.items():\n",
    "    print(f\"Variable name: {name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dicts = {**water_param_dict, **sediment_param_dict,**biomass_param_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON file\n",
    "with open('../../data/plankton-patrol_data/plank_CBPparam_dict.json', 'w') as f:\n",
    "    json.dump(param_dicts, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
