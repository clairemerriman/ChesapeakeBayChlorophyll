{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49435,"status":"ok","timestamp":1726771748819,"user":{"displayName":"Claire Merriman","userId":"10396121315400453588"},"user_tz":240},"id":"B4cTM3ojIp06","outputId":"3c5256b4-9453-4054-df12-852eabf8d675"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/ChesapeakeBay/ChesapeakeBayChlorophyll/notebooks/models\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","%cd /content/drive/MyDrive/ChesapeakeBay/ChesapeakeBayChlorophyll/notebooks/models"]},{"cell_type":"markdown","metadata":{"id":"NfAM04byIp09"},"source":["# Set up"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5726,"status":"ok","timestamp":1726771754542,"user":{"displayName":"Claire Merriman","userId":"10396121315400453588"},"user_tz":240},"id":"5lKH0EkzIp0-"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import datetime\n","import xarray as xr\n","import matplotlib.pyplot as plt\n","\n","\n","import logging\n","from tqdm import tqdm  # For progress bar\n","# Configure logging instead of print\n","logging.basicConfig(filename='tuning.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","import itertools\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","import contextlib\n","from concurrent.futures import ThreadPoolExecutor, as_completed"]},{"cell_type":"markdown","metadata":{"id":"6cvfG495Ip0_"},"source":["# Input and shape the data"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"3kikXqoeIp0_"},"outputs":[],"source":["# satellite_buoy = xr.open_dataset('../../data/satelliteBuoy_clean.nc4')\n","satellite_buoy = xr.open_dataset('../../data/satelliteBuoy_region1.nc4')"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"IYZjIwNoIp1A","outputId":"d5e2b5e3-7760-405a-dd11-bd19ebda4001"},"outputs":[{"data":{"text/html":["<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n","<defs>\n","<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n","<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n","<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n","<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n","</symbol>\n","<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n","<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n","<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n","<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n","<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n","</symbol>\n","</defs>\n","</svg>\n","<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n"," *\n"," */\n","\n",":root {\n","  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n","  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n","  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n","  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n","  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n","  --xr-background-color: var(--jp-layout-color0, white);\n","  --xr-background-color-row-even: var(--jp-layout-color1, white);\n","  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n","}\n","\n","html[theme=dark],\n","body[data-theme=dark],\n","body.vscode-dark {\n","  --xr-font-color0: rgba(255, 255, 255, 1);\n","  --xr-font-color2: rgba(255, 255, 255, 0.54);\n","  --xr-font-color3: rgba(255, 255, 255, 0.38);\n","  --xr-border-color: #1F1F1F;\n","  --xr-disabled-color: #515151;\n","  --xr-background-color: #111111;\n","  --xr-background-color-row-even: #111111;\n","  --xr-background-color-row-odd: #313131;\n","}\n","\n",".xr-wrap {\n","  display: block !important;\n","  min-width: 300px;\n","  max-width: 700px;\n","}\n","\n",".xr-text-repr-fallback {\n","  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n","  display: none;\n","}\n","\n",".xr-header {\n","  padding-top: 6px;\n","  padding-bottom: 6px;\n","  margin-bottom: 4px;\n","  border-bottom: solid 1px var(--xr-border-color);\n","}\n","\n",".xr-header > div,\n",".xr-header > ul {\n","  display: inline;\n","  margin-top: 0;\n","  margin-bottom: 0;\n","}\n","\n",".xr-obj-type,\n",".xr-array-name {\n","  margin-left: 2px;\n","  margin-right: 10px;\n","}\n","\n",".xr-obj-type {\n","  color: var(--xr-font-color2);\n","}\n","\n",".xr-sections {\n","  padding-left: 0 !important;\n","  display: grid;\n","  grid-template-columns: 150px auto auto 1fr 20px 20px;\n","}\n","\n",".xr-section-item {\n","  display: contents;\n","}\n","\n",".xr-section-item input {\n","  display: none;\n","}\n","\n",".xr-section-item input + label {\n","  color: var(--xr-disabled-color);\n","}\n","\n",".xr-section-item input:enabled + label {\n","  cursor: pointer;\n","  color: var(--xr-font-color2);\n","}\n","\n",".xr-section-item input:enabled + label:hover {\n","  color: var(--xr-font-color0);\n","}\n","\n",".xr-section-summary {\n","  grid-column: 1;\n","  color: var(--xr-font-color2);\n","  font-weight: 500;\n","}\n","\n",".xr-section-summary > span {\n","  display: inline-block;\n","  padding-left: 0.5em;\n","}\n","\n",".xr-section-summary-in:disabled + label {\n","  color: var(--xr-font-color2);\n","}\n","\n",".xr-section-summary-in + label:before {\n","  display: inline-block;\n","  content: '►';\n","  font-size: 11px;\n","  width: 15px;\n","  text-align: center;\n","}\n","\n",".xr-section-summary-in:disabled + label:before {\n","  color: var(--xr-disabled-color);\n","}\n","\n",".xr-section-summary-in:checked + label:before {\n","  content: '▼';\n","}\n","\n",".xr-section-summary-in:checked + label > span {\n","  display: none;\n","}\n","\n",".xr-section-summary,\n",".xr-section-inline-details {\n","  padding-top: 4px;\n","  padding-bottom: 4px;\n","}\n","\n",".xr-section-inline-details {\n","  grid-column: 2 / -1;\n","}\n","\n",".xr-section-details {\n","  display: none;\n","  grid-column: 1 / -1;\n","  margin-bottom: 5px;\n","}\n","\n",".xr-section-summary-in:checked ~ .xr-section-details {\n","  display: contents;\n","}\n","\n",".xr-array-wrap {\n","  grid-column: 1 / -1;\n","  display: grid;\n","  grid-template-columns: 20px auto;\n","}\n","\n",".xr-array-wrap > label {\n","  grid-column: 1;\n","  vertical-align: top;\n","}\n","\n",".xr-preview {\n","  color: var(--xr-font-color3);\n","}\n","\n",".xr-array-preview,\n",".xr-array-data {\n","  padding: 0 5px !important;\n","  grid-column: 2;\n","}\n","\n",".xr-array-data,\n",".xr-array-in:checked ~ .xr-array-preview {\n","  display: none;\n","}\n","\n",".xr-array-in:checked ~ .xr-array-data,\n",".xr-array-preview {\n","  display: inline-block;\n","}\n","\n",".xr-dim-list {\n","  display: inline-block !important;\n","  list-style: none;\n","  padding: 0 !important;\n","  margin: 0;\n","}\n","\n",".xr-dim-list li {\n","  display: inline-block;\n","  padding: 0;\n","  margin: 0;\n","}\n","\n",".xr-dim-list:before {\n","  content: '(';\n","}\n","\n",".xr-dim-list:after {\n","  content: ')';\n","}\n","\n",".xr-dim-list li:not(:last-child):after {\n","  content: ',';\n","  padding-right: 5px;\n","}\n","\n",".xr-has-index {\n","  font-weight: bold;\n","}\n","\n",".xr-var-list,\n",".xr-var-item {\n","  display: contents;\n","}\n","\n",".xr-var-item > div,\n",".xr-var-item label,\n",".xr-var-item > .xr-var-name span {\n","  background-color: var(--xr-background-color-row-even);\n","  margin-bottom: 0;\n","}\n","\n",".xr-var-item > .xr-var-name:hover span {\n","  padding-right: 5px;\n","}\n","\n",".xr-var-list > li:nth-child(odd) > div,\n",".xr-var-list > li:nth-child(odd) > label,\n",".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n","  background-color: var(--xr-background-color-row-odd);\n","}\n","\n",".xr-var-name {\n","  grid-column: 1;\n","}\n","\n",".xr-var-dims {\n","  grid-column: 2;\n","}\n","\n",".xr-var-dtype {\n","  grid-column: 3;\n","  text-align: right;\n","  color: var(--xr-font-color2);\n","}\n","\n",".xr-var-preview {\n","  grid-column: 4;\n","}\n","\n",".xr-index-preview {\n","  grid-column: 2 / 5;\n","  color: var(--xr-font-color2);\n","}\n","\n",".xr-var-name,\n",".xr-var-dims,\n",".xr-var-dtype,\n",".xr-preview,\n",".xr-attrs dt {\n","  white-space: nowrap;\n","  overflow: hidden;\n","  text-overflow: ellipsis;\n","  padding-right: 10px;\n","}\n","\n",".xr-var-name:hover,\n",".xr-var-dims:hover,\n",".xr-var-dtype:hover,\n",".xr-attrs dt:hover {\n","  overflow: visible;\n","  width: auto;\n","  z-index: 1;\n","}\n","\n",".xr-var-attrs,\n",".xr-var-data,\n",".xr-index-data {\n","  display: none;\n","  background-color: var(--xr-background-color) !important;\n","  padding-bottom: 5px !important;\n","}\n","\n",".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",".xr-var-data-in:checked ~ .xr-var-data,\n",".xr-index-data-in:checked ~ .xr-index-data {\n","  display: block;\n","}\n","\n",".xr-var-data > table {\n","  float: right;\n","}\n","\n",".xr-var-name span,\n",".xr-var-data,\n",".xr-index-name div,\n",".xr-index-data,\n",".xr-attrs {\n","  padding-left: 25px !important;\n","}\n","\n",".xr-attrs,\n",".xr-var-attrs,\n",".xr-var-data,\n",".xr-index-data {\n","  grid-column: 1 / -1;\n","}\n","\n","dl.xr-attrs {\n","  padding: 0;\n","  margin: 0;\n","  display: grid;\n","  grid-template-columns: 125px auto;\n","}\n","\n",".xr-attrs dt,\n",".xr-attrs dd {\n","  padding: 0;\n","  margin: 0;\n","  float: left;\n","  padding-right: 10px;\n","  width: auto;\n","}\n","\n",".xr-attrs dt {\n","  font-weight: normal;\n","  grid-column: 1;\n","}\n","\n",".xr-attrs dt:hover span {\n","  display: inline-block;\n","  background: var(--xr-background-color);\n","  padding-right: 10px;\n","}\n","\n",".xr-attrs dd {\n","  grid-column: 2;\n","  white-space: pre-wrap;\n","  word-break: break-all;\n","}\n","\n",".xr-icon-database,\n",".xr-icon-file-text2,\n",".xr-no-icon {\n","  display: inline-block;\n","  vertical-align: middle;\n","  width: 1em;\n","  height: 1.5em !important;\n","  stroke-width: 0;\n","  stroke: currentColor;\n","  fill: currentColor;\n","}\n","</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 37MB\n","Dimensions:          (RegionID: 1, time: 2767, depth: 1, lat: 20, lon: 12)\n","Coordinates:\n","  * time             (time) datetime64[ns] 22kB 2014-08-28T17:55:08 ... 2022-...\n","  * depth            (depth) float64 8B 0.0\n","  * lat              (lat) float64 160B 39.6 39.59 39.58 ... 39.43 39.42 39.41\n","  * lon              (lon) float64 96B -76.12 -76.11 -76.1 ... -76.0 -75.99\n","  * RegionID         (RegionID) int64 8B 1\n","Data variables:\n","    chlor_a          (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Air Temperature  (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Air pressure     (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Humidity         (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Wind speed       (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Wind Direction   (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Salinity_Zone    (RegionID, time, depth, lat, lon) float64 5MB ...\n","Attributes:\n","    STATE:       MD\n","    BasinGroup:  MD MAIN\n","    Name:        Northern Chesapeake Bay</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-7cb8cd3b-4633-47eb-97b9-d5da12d658d1' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-7cb8cd3b-4633-47eb-97b9-d5da12d658d1' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>RegionID</span>: 1</li><li><span class='xr-has-index'>time</span>: 2767</li><li><span class='xr-has-index'>depth</span>: 1</li><li><span class='xr-has-index'>lat</span>: 20</li><li><span class='xr-has-index'>lon</span>: 12</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-99e215a9-14be-434f-a81c-cea7a84daea7' class='xr-section-summary-in' type='checkbox'  checked><label for='section-99e215a9-14be-434f-a81c-cea7a84daea7' class='xr-section-summary' >Coordinates: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2014-08-28T17:55:08 ... 2022-10-...</div><input id='attrs-c8e0cd3c-5b35-414a-a83b-66e353ca8d7b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c8e0cd3c-5b35-414a-a83b-66e353ca8d7b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ef671bc2-5e15-4d87-a415-27686e50d523' class='xr-var-data-in' type='checkbox'><label for='data-ef671bc2-5e15-4d87-a415-27686e50d523' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;2014-08-28T17:55:08.000000000&#x27;, &#x27;2014-08-29T17:50:07.500000000&#x27;,\n","       &#x27;2014-08-30T17:42:38.000000000&#x27;, ..., &#x27;2022-10-28T18:05:09.500000000&#x27;,\n","       &#x27;2022-10-29T17:57:40.500000000&#x27;, &#x27;2022-10-30T17:52:40.000000000&#x27;],\n","      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>depth</span></div><div class='xr-var-dims'>(depth)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.0</div><input id='attrs-3b7a43b2-0c9e-42a9-90f2-94b6af65c71a' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-3b7a43b2-0c9e-42a9-90f2-94b6af65c71a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-98d9a9d7-4039-45a0-b5d9-ca26024a2f6e' class='xr-var-data-in' type='checkbox'><label for='data-98d9a9d7-4039-45a0-b5d9-ca26024a2f6e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>39.6 39.59 39.58 ... 39.42 39.41</div><input id='attrs-eb65855a-db6a-425b-b161-f529b5f434c8' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-eb65855a-db6a-425b-b161-f529b5f434c8' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-fabf9435-a93c-406c-a991-2278ef81b556' class='xr-var-data-in' type='checkbox'><label for='data-fabf9435-a93c-406c-a991-2278ef81b556' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>units :</span></dt><dd>degrees_north</dd></dl></div><div class='xr-var-data'><pre>array([39.601124, 39.591464, 39.581802, 39.57214 , 39.55281 , 39.543143,\n","       39.533475, 39.523805, 39.514134, 39.504462, 39.494788, 39.485113,\n","       39.475437, 39.465759, 39.45608 , 39.4464  , 39.436718, 39.427035,\n","       39.41735 , 39.407664])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-76.12 -76.11 ... -76.0 -75.99</div><input id='attrs-6aee12a4-692c-4901-b101-1a311ee72d88' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-6aee12a4-692c-4901-b101-1a311ee72d88' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8ad5ee3d-3bd6-489b-80be-cc231a8e8740' class='xr-var-data-in' type='checkbox'><label for='data-8ad5ee3d-3bd6-489b-80be-cc231a8e8740' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>units :</span></dt><dd>degrees_east</dd></dl></div><div class='xr-var-data'><pre>array([-76.12445 , -76.111963, -76.099477, -76.08699 , -76.074504, -76.062017,\n","       -76.04953 , -76.037044, -76.024557, -76.012071, -75.999584, -75.987097])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>RegionID</span></div><div class='xr-var-dims'>(RegionID)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>1</div><input id='attrs-7dddc62d-0a5a-47c4-ab20-ed6e1a1eda36' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-7dddc62d-0a5a-47c4-ab20-ed6e1a1eda36' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ddc5220a-e4a8-41d1-90a3-bdc7bcd68627' class='xr-var-data-in' type='checkbox'><label for='data-ddc5220a-e4a8-41d1-90a3-bdc7bcd68627' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([1])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-1e0ec44e-2e01-4317-9909-fff4129320c9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-1e0ec44e-2e01-4317-9909-fff4129320c9' class='xr-section-summary' >Data variables: <span>(7)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>chlor_a</span></div><div class='xr-var-dims'>(RegionID, time, depth, lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-f214552f-9a5c-4c4e-b911-03df3106a464' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f214552f-9a5c-4c4e-b911-03df3106a464' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6ebd62c2-6c9b-4a51-9dab-9092f2ed439e' class='xr-var-data-in' type='checkbox'><label for='data-6ebd62c2-6c9b-4a51-9dab-9092f2ed439e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[664080 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>Air Temperature</span></div><div class='xr-var-dims'>(RegionID, time, depth, lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-701c13a8-503e-40c6-bf76-aee6215469f4' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-701c13a8-503e-40c6-bf76-aee6215469f4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-410bfb53-2f4f-439b-aa20-af35b731cdd1' class='xr-var-data-in' type='checkbox'><label for='data-410bfb53-2f4f-439b-aa20-af35b731cdd1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[664080 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>Air pressure</span></div><div class='xr-var-dims'>(RegionID, time, depth, lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-11061106-4054-41ea-a68f-db147ba6a8db' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-11061106-4054-41ea-a68f-db147ba6a8db' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-92bab65b-adf9-4acd-aa71-9dcc40984eb0' class='xr-var-data-in' type='checkbox'><label for='data-92bab65b-adf9-4acd-aa71-9dcc40984eb0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[664080 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>Humidity</span></div><div class='xr-var-dims'>(RegionID, time, depth, lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-f407b119-51e1-4d6a-aacb-c3d3d557df9f' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f407b119-51e1-4d6a-aacb-c3d3d557df9f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-29724365-e887-4434-8958-08820e706404' class='xr-var-data-in' type='checkbox'><label for='data-29724365-e887-4434-8958-08820e706404' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[664080 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>Wind speed</span></div><div class='xr-var-dims'>(RegionID, time, depth, lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-770264a8-46c9-4808-83e9-ebac073d25b5' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-770264a8-46c9-4808-83e9-ebac073d25b5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3a88b2ef-5435-4370-8e8e-0bf5573c503b' class='xr-var-data-in' type='checkbox'><label for='data-3a88b2ef-5435-4370-8e8e-0bf5573c503b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[664080 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>Wind Direction</span></div><div class='xr-var-dims'>(RegionID, time, depth, lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-66e9522d-3291-4eee-af03-669b6955d40c' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-66e9522d-3291-4eee-af03-669b6955d40c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-461218fe-2c3d-4a02-8da0-4965af3c2f46' class='xr-var-data-in' type='checkbox'><label for='data-461218fe-2c3d-4a02-8da0-4965af3c2f46' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[664080 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>Salinity_Zone</span></div><div class='xr-var-dims'>(RegionID, time, depth, lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-f35a2e0e-6d84-4527-adbd-49a7ef5ccf18' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f35a2e0e-6d84-4527-adbd-49a7ef5ccf18' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7de6ccae-4530-40b9-ad83-28f0e4b760a1' class='xr-var-data-in' type='checkbox'><label for='data-7de6ccae-4530-40b9-ad83-28f0e4b760a1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[664080 values with dtype=float64]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-4ca331c2-14b7-45d1-89dc-bd5a49de6b7f' class='xr-section-summary-in' type='checkbox'  ><label for='section-4ca331c2-14b7-45d1-89dc-bd5a49de6b7f' class='xr-section-summary' >Indexes: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-6a9fe1bf-4aea-4127-8be1-4b8b6de4a4e2' class='xr-index-data-in' type='checkbox'/><label for='index-6a9fe1bf-4aea-4127-8be1-4b8b6de4a4e2' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([       &#x27;2014-08-28 17:55:08&#x27;, &#x27;2014-08-29 17:50:07.500000&#x27;,\n","                      &#x27;2014-08-30 17:42:38&#x27;,        &#x27;2014-08-31 17:40:08&#x27;,\n","               &#x27;2014-09-01 18:17:37.500000&#x27;, &#x27;2014-09-02 18:15:08.500000&#x27;,\n","                      &#x27;2014-09-03 18:07:38&#x27;, &#x27;2014-09-04 18:00:07.500000&#x27;,\n","               &#x27;2014-09-05 17:55:08.500000&#x27;,        &#x27;2014-09-06 17:50:08&#x27;,\n","               ...\n","                      &#x27;2022-10-21 18:00:10&#x27;, &#x27;2022-10-22 17:55:09.500000&#x27;,\n","               &#x27;2022-10-23 17:50:10.500000&#x27;, &#x27;2022-10-24 18:25:10.500000&#x27;,\n","               &#x27;2022-10-25 18:25:09.500000&#x27;, &#x27;2022-10-26 18:17:40.500000&#x27;,\n","                      &#x27;2022-10-27 18:10:10&#x27;, &#x27;2022-10-28 18:05:09.500000&#x27;,\n","               &#x27;2022-10-29 17:57:40.500000&#x27;,        &#x27;2022-10-30 17:52:40&#x27;],\n","              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, length=2767, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>depth</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-c2bc6623-d3cb-4425-af60-962dcf699e98' class='xr-index-data-in' type='checkbox'/><label for='index-c2bc6623-d3cb-4425-af60-962dcf699e98' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([0.0], dtype=&#x27;float64&#x27;, name=&#x27;depth&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>lat</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-6faa3064-7c42-4496-b932-57a77e69c571' class='xr-index-data-in' type='checkbox'/><label for='index-6faa3064-7c42-4496-b932-57a77e69c571' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([  39.6011240055394,  39.59146391182248, 39.581802460210234,\n","       39.572139650784244,  39.55280995881816, 39.543143076442036,\n","        39.53347483658013,  39.52380523931486, 39.514134284728804,\n","        39.50446197290469, 39.494788303925446, 39.485113277874156,\n","        39.47543689483406,  39.46575915488856, 39.456080058121266,\n","        39.44639960461591,  39.43671779445642,  39.42703462772688,\n","        39.41735010451155,  39.40766422489484],\n","      dtype=&#x27;float64&#x27;, name=&#x27;lat&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>lon</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-a5541dce-1c75-43ff-8b4f-b1470f32c634' class='xr-index-data-in' type='checkbox'/><label for='index-a5541dce-1c75-43ff-8b4f-b1470f32c634' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([ -76.1244499019218, -76.11196331947255, -76.09947673702328,\n","       -76.08699015457402, -76.07450357212475,  -76.0620169896755,\n","       -76.04953040722624, -76.03704382477697, -76.02455724232772,\n","       -76.01207065987846, -75.99958407742919, -75.98709749497993],\n","      dtype=&#x27;float64&#x27;, name=&#x27;lon&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>RegionID</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-10df92b2-9f54-463b-8df8-5a9b0be10c33' class='xr-index-data-in' type='checkbox'/><label for='index-10df92b2-9f54-463b-8df8-5a9b0be10c33' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([1], dtype=&#x27;int64&#x27;, name=&#x27;RegionID&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-4d96e38b-e2ce-46cf-aa33-eb8c41cc7ff7' class='xr-section-summary-in' type='checkbox'  checked><label for='section-4d96e38b-e2ce-46cf-aa33-eb8c41cc7ff7' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>STATE :</span></dt><dd>MD</dd><dt><span>BasinGroup :</span></dt><dd>MD MAIN</dd><dt><span>Name :</span></dt><dd>Northern Chesapeake Bay</dd></dl></div></li></ul></div></div>"],"text/plain":["<xarray.Dataset> Size: 37MB\n","Dimensions:          (RegionID: 1, time: 2767, depth: 1, lat: 20, lon: 12)\n","Coordinates:\n","  * time             (time) datetime64[ns] 22kB 2014-08-28T17:55:08 ... 2022-...\n","  * depth            (depth) float64 8B 0.0\n","  * lat              (lat) float64 160B 39.6 39.59 39.58 ... 39.43 39.42 39.41\n","  * lon              (lon) float64 96B -76.12 -76.11 -76.1 ... -76.0 -75.99\n","  * RegionID         (RegionID) int64 8B 1\n","Data variables:\n","    chlor_a          (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Air Temperature  (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Air pressure     (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Humidity         (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Wind speed       (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Wind Direction   (RegionID, time, depth, lat, lon) float64 5MB ...\n","    Salinity_Zone    (RegionID, time, depth, lat, lon) float64 5MB ...\n","Attributes:\n","    STATE:       MD\n","    BasinGroup:  MD MAIN\n","    Name:        Northern Chesapeake Bay"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["satellite_buoy"]},{"cell_type":"markdown","metadata":{"id":"zwjTxnE0Ip1B"},"source":["We need to make sure the timesteps are in order."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Ag0zPuoUIp1C","outputId":"e20dd298-ad89-4168-94e9-970e7919cda6"},"outputs":[{"name":"stdout","output_type":"stream","text":["The time steps are already in order.\n"]}],"source":["# Check if the time dimension is sorted\n","is_sorted = (satellite_buoy['time'].values == sorted(satellite_buoy['time'].values)).all()\n","\n","if is_sorted:\n","    print(\"The time steps are already in order.\")\n","else:\n","    print(\"The time steps are not in order.\")\n"]},{"cell_type":"markdown","metadata":{"id":"p69qiDS6gZ_Z"},"source":["We want to target the chorophyll measurements and use Air Temperature, Air pressure, Humidity, Wind speed, and Wind Direction as features. Let's create the target and features tensor, then save the files for use on the GPU. Since `xarray` is on CPU, this step is MUCH faster on CPU."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"vLSWlp5MIp1D","outputId":"915ad622-d046-41f9-8a19-58df79d4ac0a"},"outputs":[],"source":["# Define the variables to be concatenated into features\n","variables_to_expand = ['Air Temperature', 'Air pressure', 'Humidity', 'Wind speed', 'Wind Direction','Salinity_Zone']\n","\n","# Convert xarray combined features to a PyTorch tensor (on CPU)\n","features = xr.concat([satellite_buoy[var] for var in variables_to_expand], dim='variable')\n","features_tensor = torch.tensor(features.values, dtype=torch.float32)\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"ename":"IndexError","evalue":"index 1 is out of bounds for dimension 3 with size 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeatures_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n","\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 3 with size 1"]}],"source":["features_tensor[:,:,:,1,:]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Rearrange the dimensions to match LSTM input format: (region,time_steps, features, depth, lat, lon)\n","features_tensor = features_tensor.permute(1, 0, 2, 3, 4)\n","\n","# Convert 'chlor_a' to a PyTorch tensor (target) and add depth dimension if needed\n","chlorophyll_tensor = torch.tensor(satellite_buoy['chlor_a'].values, dtype=torch.float32)\n","\n","# Replace NaN values with -1\n","chlorophyll_tensor[torch.isnan(chlorophyll_tensor)] = -1\n","\n","# Now save both tensors as .pt files for later use\n","torch.save(features_tensor, '../../data/features_tensor.pt')\n","torch.save(chlorophyll_tensor, '../../data/chlorophyll_tensor.pt')\n","\n","# Optional: Print shapes to confirm\n","print(f\"Features tensor shape: {features_tensor.shape}\")\n","print(f\"Chlorophyll tensor shape: {chlorophyll_tensor.shape}\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"executionInfo":{"elapsed":280,"status":"error","timestamp":1726771783536,"user":{"displayName":"Claire Merriman","userId":"10396121315400453588"},"user_tz":240},"id":"cbEyRKCTgZ_a","outputId":"4877789c-b793-4699-ef05-e200daf177f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded Features tensor shape: torch.Size([2767, 5, 1, 273, 66])\n","Loaded Chlorophyll tensor shape: torch.Size([2767, 1, 273, 66])\n"]}],"source":["# Load tensors\n","features_tensor = torch.load('../../data/features_masked_tensor.pt')\n","chlorophyll_tensor = torch.load('../../data/chlorophyll_masked_tensor.pt')\n","\n","# Optional: Print shapes to confirm\n","print(f\"Loaded Features tensor shape: {features_tensor.shape}\")\n","print(f\"Loaded Chlorophyll tensor shape: {chlorophyll_tensor.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"elw_9KwpIp1D"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"1JjbhYbjIp1D"},"source":["## Defining the classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TqW4f2jxIp1D"},"outputs":[],"source":["class ConvLSTMCell(nn.Module):\n","    # For one time slice\n","    # input has shape (batch_size, features, depth, latitude, longitude)\n","    def __init__(self, input_channels, hidden_channels, kernel_size):\n","        super(ConvLSTMCell, self).__init__()\n","        padding = kernel_size // 2\n","        self.conv = nn.Conv3d(input_channels + hidden_channels,\n","                              hidden_channels * 4,  # 4 for i, f, o, g gates\n","                              kernel_size,\n","                              padding=padding)\n","\n","    def forward(self, input_tensor, hidden_state):\n","        h_cur, c_cur = hidden_state\n","\n","        # Concatenate input and hidden state\n","        combined = torch.cat([input_tensor, h_cur], dim=1).contiguous()\n","\n","        conv_output = self.conv(combined)\n","\n","        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, conv_output.shape[1] // 4, dim=1)\n","\n","        i = torch.sigmoid(cc_i)\n","        f = torch.sigmoid(cc_f)\n","        o = torch.sigmoid(cc_o)\n","        g = torch.tanh(cc_g)\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * torch.tanh(c_next)\n","\n","        return h_next, c_next"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AoQrWWSzIp1E"},"outputs":[],"source":["class ConvLSTM(nn.Module):\n","    def __init__(self, input_channels, hidden_channels, kernel_size, num_layers, output_channels=1):\n","        super(ConvLSTM, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_channels = hidden_channels\n","\n","        # Define a list of ConvLSTM cells\n","        self.lstm_cells = nn.ModuleList([\n","            ConvLSTMCell(input_channels if i == 0 else hidden_channels,  # First layer uses input channels, others use hidden channels\n","                         hidden_channels,\n","                         kernel_size)\n","            for i in range(num_layers)\n","        ])\n","\n","\n","        self.conv = nn.Conv3d(in_channels=hidden_channels, out_channels=1, kernel_size=1)\n","\n","\n","    def forward(self, input_tensor, time_step_batch_size=10):\n","        # if no batch_size dimension, add one\n","        if len(input_tensor.shape) == 5:\n","            input_tensor = input_tensor.unsqueeze(0)\n","\n","        batch_size, time_steps, channels, depth, height, width = input_tensor.size()\n","        h, c = self.init_hidden(batch_size, depth, height, width, input_tensor.device)\n","\n","        output_inner = []\n","\n","        # Process time steps in batches\n","        for t in range(0, time_steps, time_step_batch_size):\n","            # Select a batch of time steps to process\n","            time_step_batch = input_tensor[:, t:t + time_step_batch_size, :, :, :, :]\n","            for t_batch in range(time_step_batch.size(1)):  # Iterate over the time step batch\n","                x = time_step_batch[:, t_batch, :, :, :, :]  # Start with the input tensor\n","                for i, cell in enumerate(self.lstm_cells):\n","                    h[i], c[i] = cell(x, (h[i], c[i]))  # Pass hidden state to the next layer\n","                    x = h[i]  # The output of this layer becomes input for the next layer\n","\n","\n","\n","                output_inner.append(h[-1])  # Save the output from the final layer\n","\n","        output = torch.stack(output_inner, dim=1)  # Stack outputs across time steps\n","        output = output.squeeze(0)  # Remove the batch dimension to get (time, hidden_channels, depth, lat, lon)\n","        # Turn hidden_layers into a predicted value\n","        final_output = self.conv(output)\n","\n","\n","        return final_output\n","\n","    # Define hidden state initialization\n","    def init_hidden(self, batch_size, depth, height, width, device):\n","        h = [torch.zeros(batch_size, self.hidden_channels, depth, height, width).to(device) for _ in range(self.num_layers)]\n","        c = [torch.zeros(batch_size, self.hidden_channels, depth, height, width).to(device) for _ in range(self.num_layers)]\n","        return h, c\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10XMsEGSIp1F"},"outputs":[],"source":["class HyperparameterTuner:\n","    def __init__(self, input_channels, train_data, val_data, criterion, device):\n","        self.input_channels = input_channels\n","        self.train_features, self.train_targets = train_data\n","        self.val_features, self.val_targets = val_data\n","        self.criterion = criterion\n","        self.device = device\n","\n","    def build_model(self, hidden_channels, kernel_size, num_layers):\n","        model = ConvLSTM(\n","            input_channels=self.input_channels,\n","            hidden_channels=hidden_channels,\n","            kernel_size=kernel_size,\n","            num_layers=num_layers\n","        ).to(self.device)\n","        return model\n","\n","    def _run_one_epoch(self, model, features, targets, optimizer, scaler, training=True):\n","        # Set model mode: training or evaluation\n","        model.train() if training else model.eval()\n","\n","        # Use autocast and gradients only if running on a GPU\n","        use_cuda = torch.cuda.is_available()\n","        with torch.set_grad_enabled(training), (torch.cuda.amp.autocast() if use_cuda else contextlib.nullcontext()):\n","            output = model(features)\n","            predicted_output = output[:, 0, :, :, :]  # Select the first hidden channel\n","            loss = self.criterion(predicted_output, targets)\n","\n","        # Backward pass and optimization (only during training)\n","        if training:\n","            optimizer.zero_grad()  # Zero out gradients before backward pass\n","            if use_cuda and scaler:\n","                scaler.scale(loss).backward()  # Backward pass with mixed precision scaling\n","                scaler.step(optimizer)\n","                scaler.update()  # Update the scaler for AMP\n","            else:\n","                loss.backward()\n","                optimizer.step()\n","\n","        return loss.item()\n","\n","    def _train_single_config(self, hidden_channels, kernel_size, num_layers, lr, epochs, pbar):\n","        # Build the model\n","        model = self.build_model(hidden_channels, kernel_size, num_layers)\n","\n","        # Ensure model is moved to the correct device (CPU or GPU)\n","        model = model.to(self.device)\n","\n","        # Set up the optimizer\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","        # Train the model and capture losses\n","        train_losses, val_losses = self.train(model, optimizer, epochs)\n","\n","        # Free up memory manually after each model is trained\n","        torch.cuda.empty_cache()  # Free memory if on GPU\n","\n","        # Update progress bar\n","        pbar.update(1)\n","\n","        return val_losses[-1], hidden_channels, kernel_size, num_layers, lr, model\n","\n","    def train(self, model, optimizer, epochs, early_stopping_patience=5):\n","        train_losses = []\n","        val_losses = []\n","        best_val_loss = float('inf')\n","        patience_counter = 0\n","\n","        # Initialize mixed precision scaler only if GPU is available\n","        scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n","\n","\n","        for epoch in range(epochs):\n","            # TRAINING PHASE\n","            train_loss = self._run_one_epoch(model, self.train_features, self.train_targets, optimizer, scaler, training=True)\n","            train_losses.append(train_loss)\n","\n","            # VALIDATION PHASE\n","            val_loss = self._run_one_epoch(model, self.val_features, self.val_targets, optimizer, scaler, training=False)\n","            val_losses.append(val_loss)\n","\n","            # Check for early stopping\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","\n","            if patience_counter >= early_stopping_patience:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","        return train_losses, val_losses\n","\n","    def tune(self, hidden_channels_list, kernel_size_list, num_layers_list, lr_list, epochs=5):\n","        best_val_loss = float('inf')\n","        best_config = None\n","        best_model = None\n","\n","        # Create the product of all hyperparameter combinations\n","        hyperparameter_combinations = list(itertools.product(\n","            hidden_channels_list, kernel_size_list, num_layers_list, lr_list\n","        ))\n","\n","        # Initialize the progress bar\n","        with tqdm(total=len(hyperparameter_combinations), desc=\"Hyperparameter Tuning\", leave=True) as pbar:\n","            with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust the number of workers as needed\n","                futures = []\n","                for hidden_channels, kernel_size, num_layers, lr in hyperparameter_combinations:\n","                    # Submit each combination to the ThreadPoolExecutor\n","                    futures.append(executor.submit(self._train_single_config, hidden_channels, kernel_size, num_layers, lr, epochs, pbar))\n","\n","                # Process each future as it completes\n","                for future in as_completed(futures):\n","                    result = future.result()\n","                    val_loss, hidden_channels, kernel_size, num_layers, lr, model = result\n","\n","                    # Check for the best configuration\n","                    if val_loss < best_val_loss:\n","                        best_val_loss = val_loss\n","                        best_config = (hidden_channels, kernel_size, num_layers, lr)\n","                        best_model = model\n","\n","        # Log the best configuration\n","        print(f\"Best config: hidden_channels={best_config[0]}, kernel_size={best_config[1]}, num_layers={best_config[2]}, lr={best_config[3]}\")\n","        print(f\"Best validation loss: {best_val_loss}\")\n","\n","        return best_model, best_config\n"]},{"cell_type":"markdown","metadata":{"id":"FfVrv7aFIp1G"},"source":["We also define a function to run the model on the trainig values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpwW_fvbIp1G"},"outputs":[],"source":["def test_model(model, test_features, test_targets, criterion):\n","    model.eval()  # Set the model to evaluation mode\n","\n","    with torch.no_grad():  # Disable gradient calculations for testing\n","        test_output = model(test_features)  # Forward pass on the test data\n","\n","        # Ensure we are selecting the correct dimensions from the model output\n","        # Assuming that the output is of shape (time, 1, depth, lat, lon), so we squeeze to remove the singleton dimension\n","        predicted_chlorophyll_test = test_output.squeeze(1)  # Shape should be (time, depth, lat, lon)\n","\n","        # Calculate the test loss\n","        test_loss = criterion(predicted_chlorophyll_test, test_targets)\n","\n","    print(f\"Test Loss: {test_loss.item()}\")\n","\n","    return predicted_chlorophyll_test\n"]},{"cell_type":"markdown","metadata":{"id":"eAgFobgIIp1G"},"source":["## Small test on CPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_m6evErIp1G","outputId":"0c834a51-8c7b-42e4-d081-1e2cac939878"},"outputs":[{"name":"stdout","output_type":"stream","text":["Chlorophyll test tensor shape: torch.Size([15, 1, 50, 50])\n","Features test tensor shape: torch.Size([15, 5, 1, 50, 50])\n"]}],"source":["# Define the small test dataset dimensions\n","test_time_steps = 15  # Use only 15 time steps\n","test_lat_range = slice(0, 50)  # Use the first 50 latitude values\n","test_lon_range = slice(0, 50)  # Use the first 50 longitude values\n","\n","# Slice the data to get a smaller subset for the test\n","chlorophyll_test_tensor = chlorophyll_tensor[:test_time_steps, :, test_lat_range, test_lon_range]\n","features_test_tensor = features_tensor[:test_time_steps, :, :, test_lat_range, test_lon_range]\n","\n","# Print shapes to verify\n","print(f\"Chlorophyll test tensor shape: {chlorophyll_test_tensor.shape}\")\n","print(f\"Features test tensor shape: {features_test_tensor.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YjCykpDIp1H"},"outputs":[],"source":["# Split data into 70% training, 15% validation, 15% test\n","train_size = int(0.7 * features_test_tensor.shape[0])  # Shape[0] corresponds to time_steps\n","val_size = int(0.15 * features_test_tensor.shape[0])    # 15% for validation\n","test_size = features_test_tensor.shape[0] - train_size - val_size  # Remaining for test set\n","\n","# Split features into train, validation, and test sets\n","train_features = features_test_tensor[:train_size, :, :, :, :]   # First 70% of time steps for training\n","val_features = features_test_tensor[train_size:train_size+val_size, :, :, :, :]  # Next 15% for validation\n","test_features = features_test_tensor[train_size+val_size:, :, :, :, :]  # Last 15% for test\n","\n","# Split targets (chlorophyll) accordingly\n","train_targets = chlorophyll_test_tensor[:train_size, :, :, :]    # First 70% of chlorophyll targets for training\n","val_targets = chlorophyll_test_tensor[train_size:train_size+val_size, :, :, :]  # Next 15% for validation\n","test_targets = chlorophyll_test_tensor[train_size+val_size:, :, :, :]  # Last 15% for test\n"]},{"cell_type":"markdown","metadata":{"id":"WTo-5wLEIp1H"},"source":["Testing without the tuner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61EpoHFGIp1H"},"outputs":[],"source":["# Set parameters for ConvLSTM\n","batch_size = 1\n","input_channels = features_test_tensor.shape[1]  # Number of features (channels)\n","hidden_channels = 16  # Set a small number of hidden channels for the test\n","kernel_size = 3  # Use a small kernel size\n","num_layers = 2  # Test with a single layer for now\n","\n","# Initialize the ConvLSTM model\n","conv_lstm = ConvLSTM(input_channels=input_channels,\n","                     hidden_channels=hidden_channels,\n","                     kernel_size=kernel_size,\n","                     num_layers=num_layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vj-oIS3aIp1H","outputId":"af260c91-1872-4835-f55a-29d2c62ccee8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/5], Train Loss: 1.130357027053833, Val Loss: 1.1132137775421143\n","Epoch [2/5], Train Loss: 1.111569881439209, Val Loss: 1.1037242412567139\n","Epoch [3/5], Train Loss: 1.0929533243179321, Val Loss: 1.0931822061538696\n","Epoch [4/5], Train Loss: 1.0711796283721924, Val Loss: 1.0808534622192383\n","Epoch [5/5], Train Loss: 1.04345703125, Val Loss: 1.0663129091262817\n","Training and validation complete!\n"]}],"source":["# Define the loss function (Mean Squared Error for regression tasks)\n","criterion = nn.MSELoss()\n","\n","# Define the optimizer\n","optimizer = optim.Adam(conv_lstm.parameters(), lr=0.001)\n","\n","# Set the number of training epochs\n","epochs = 5  # You can adjust the number of epochs\n","\n","# Training loop with validation\n","for epoch in range(epochs):\n","    # TRAINING PHASE\n","    conv_lstm.train()  # Set the model to training mode\n","\n","    optimizer.zero_grad()  # Zero out the gradients\n","\n","    # Forward pass (training)\n","    train_output = conv_lstm(train_features)\n","    predicted_chlorophyll_train = train_output[:, 0, :, :, :]  # Select the first hidden channel\n","\n","    # Calculate training loss\n","    train_loss = criterion(predicted_chlorophyll_train, train_targets)\n","\n","    # Backward pass and optimization\n","    train_loss.backward()\n","    optimizer.step()\n","\n","    # VALIDATION PHASE\n","    conv_lstm.eval()  # Set the model to evaluation mode\n","    with torch.no_grad():  # Disable gradient computation for validation\n","        val_output = conv_lstm(val_features)\n","        predicted_chlorophyll_val = val_output[:, 0, :, :, :]  # Select the first hidden channel\n","\n","        # Calculate validation loss\n","        val_loss = criterion(predicted_chlorophyll_val, val_targets)\n","\n","    # Print training and validation loss for the current epoch\n","    print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss.item()}, Val Loss: {val_loss.item()}\")\n","\n","print(\"Training and validation complete!\")\n"]},{"cell_type":"markdown","metadata":{"id":"0394haPYIp1H"},"source":["Now we can apply our hypertuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPO6JLhrIp1I","outputId":"17ef00f6-cf29-40ac-bd0f-f4ff0a49d286"},"outputs":[{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning: 100%|██████████| 24/24 [00:35<00:00,  1.48s/it]"]},{"name":"stdout","output_type":"stream","text":["Best config: hidden_channels=16, kernel_size=5, num_layers=1, lr=0.001\n","Best validation loss: 0.24294668436050415\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Prepare your hyperparameter ranges\n","hidden_channels_list = [8, 16, 32]\n","kernel_size_list = [3, 5]\n","num_layers_list = [1, 2]\n","lr_list = [0.001, 0.0005]\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Create an instance of HyperparameterTuner with your prepared data\n","tuner = HyperparameterTuner(\n","    input_channels=train_features.shape[1],  # Ensure this is the correct number of channels\n","    train_data=(train_features, train_targets),  # Training features and targets\n","    val_data=(val_features, val_targets),  # Validation features and targets\n","    criterion=nn.MSELoss(),  # Loss function\n","    device=device  # Device to run on (CPU or GPU)\n",")\n","\n","# Call the tuning method\n","best_model, best_config = tuner.tune(\n","    hidden_channels_list=hidden_channels_list,\n","    kernel_size_list=kernel_size_list,\n","    num_layers_list=num_layers_list,\n","    lr_list=lr_list,\n","    epochs=5  # Number of epochs for each tuning iteration\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JuMV7i3lIp1I","outputId":"d94c91de-670a-4325-b236-5fe9637befe1"},"outputs":[{"data":{"text/plain":["ConvLSTM(\n","  (lstm_cells): ModuleList(\n","    (0): ConvLSTMCell(\n","      (conv): Conv3d(37, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n","    )\n","  )\n","  (conv): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",")"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bb9AiT0DIp1I","outputId":"dcbdac31-5699-45f8-8be3-c0ec449b7bee"},"outputs":[{"ename":"NameError","evalue":"name 'test_model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predicted_chlorophyll_test \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m(best_model, test_features, test_targets, criterion)\n","\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"]}],"source":["predicted_chlorophyll_test = test_model(best_model, test_features, test_targets, criterion)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFOqr-r7Ip1J"},"outputs":[],"source":["plt.figure(figsize=(10, 5))\n","plt.plot(test_targets[0, :, :, :].flatten().cpu().numpy(), label='True Values')\n","plt.plot(predicted_chlorophyll_test[0, :, :, :].flatten().cpu().numpy(), label='Predicted Values')\n","plt.legend()\n","plt.title('Predicted vs. Actual Chlorophyll Concentrations (Test Data)')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"rLUDyRxjIp1J"},"source":["## Spit the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183,"status":"ok","timestamp":1726748772318,"user":{"displayName":"Claire Merriman","userId":"10396121315400453588"},"user_tz":240},"id":"rDSux8ZAIp1J","outputId":"72eaeb5d-7514-4d3e-9287-3e71981de945"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train features shape: torch.Size([1936, 5, 1, 358, 243])\n","Train target shape: torch.Size([1936, 1, 358, 243])\n","Validation features shape: torch.Size([554, 5, 1, 358, 243])\n","Validation target shape: torch.Size([554, 1, 358, 243])\n","Test features shape: torch.Size([277, 5, 1, 358, 243])\n","Test target shape: torch.Size([277, 1, 358, 243])\n"]}],"source":["# Define the split percentages\n","train_split = 0.6\n","val_split = 0.2\n","test_split = 1 - train_split - val_split\n","\n","# Number of time steps\n","n_time_steps = features_tensor.shape[0]\n","\n","# Indices for the splits (70% train, 20% validation, 10% test)\n","train_idx = int(n_time_steps * 0.7)  # 70% for training\n","val_idx = int(n_time_steps * (0.7 + 0.2))  # 20% for validation, 10% for testing\n","\n","# Split the features and target tensors along the time dimension\n","train_features = features_tensor[:train_idx, :, :, :, :]\n","train_target = chlorophyll_tensor[:train_idx, :, :, :]\n","\n","val_features = features_tensor[train_idx:val_idx, :, :, :, :]\n","val_target = chlorophyll_tensor[train_idx:val_idx, :, :, :]\n","\n","test_features = features_tensor[val_idx:, :, :, :, :]\n","test_target = chlorophyll_tensor[val_idx:, :, :, :]\n","\n","\n","# Print shapes to verify correctness\n","print(f\"Train features shape: {train_features.shape}\")\n","print(f\"Train target shape: {train_target.shape}\")\n","print(f\"Validation features shape: {val_features.shape}\")\n","print(f\"Validation target shape: {val_target.shape}\")\n","print(f\"Test features shape: {test_features.shape}\")\n","print(f\"Test target shape: {test_target.shape}\")\n"]},{"cell_type":"markdown","metadata":{"id":"5gdPcQ5TIp1J"},"source":["Now move to the GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElxM5PcCIp1N"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move data to GPU (only when necessary)\n","train_features = train_features.to(device)\n","train_target = train_target.to(device)\n","val_features = val_features.to(device)\n","val_target = val_target.to(device)\n","test_features = test_features.to(device)\n","test_target = test_target.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ld_4MAigiJin"},"outputs":[],"source":["!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzGlOoUwIp1N","outputId":"cd12665a-d121-4bec-b832-91f21fa2f6a5"},"outputs":[{"name":"stderr","output_type":"stream","text":["\rHyperparameter Tuning:   0%|          | 0/24 [00:00<?, ?it/s]"]}],"source":["# Prepare your hyperparameter ranges\n","hidden_channels_list = [8, 16, 32]\n","kernel_size_list = [3, 5]\n","num_layers_list = [1, 2]\n","lr_list = [0.001, 0.0005]\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Create an instance of HyperparameterTuner with your prepared data\n","tuner = HyperparameterTuner(\n","    input_channels=train_features.shape[1],  # Ensure this is the correct number of channels\n","    train_data=(train_features, train_target),  # Training features and targets\n","    val_data=(val_features, val_target),  # Validation features and targets\n","    criterion=nn.MSELoss(),  # Loss function\n","    device=device  # Device to run on (CPU or GPU)\n",")\n","\n","# Call the tuning method\n","best_model, best_config = tuner.tune(\n","    hidden_channels_list=hidden_channels_list,\n","    kernel_size_list=kernel_size_list,\n","    num_layers_list=num_layers_list,\n","    lr_list=lr_list,\n","    epochs=5  # Number of epochs for each tuning iteration\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ipp1rjskgZ_h"},"outputs":[],"source":["predicted_chlorophyll_test = test_model(best_model, test_features, test_targets, criterion)\n","# Assuming predicted_chlorophyll_test is your model's output and test_targets are the true values\n","plt.figure(figsize=(10, 5))\n","plt.plot(test_targets[0, :, :, :].flatten().cpu().numpy(), label='True Values')\n","plt.plot(predicted_chlorophyll_test[0, :, :, :].flatten().cpu().numpy(), label='Predicted Values')\n","plt.legend()\n","plt.title('Predicted vs. Actual Chlorophyll Concentrations (Test Data)')\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
